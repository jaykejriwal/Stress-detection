{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaldiio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95937e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340e7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaldiio\n",
    "import pandas as pd\n",
    "\n",
    "temp = []\n",
    "temp1 = []\n",
    "dictlist = []\n",
    "dictlist1 = []\n",
    "d = kaldiio.load_ark('StressDat_xvector512.ark')  # d is a generator object\n",
    "for key, numpy_array in d:\n",
    "#    print(key)\n",
    "#    print(numpy_array)\n",
    "    temp = key\n",
    "    temp1 = numpy_array\n",
    "    dictlist.append(temp)\n",
    "    dictlist1.append(temp1)\n",
    "df = pd.DataFrame(list(zip(dictlist, dictlist1)),\n",
    "               columns =['FILE', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb63d06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s01a_11a_crisis_1_0298</td>\n",
       "      <td>[0.5722328, -0.5864534, 0.4016994, -0.532431, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s01a_11a_crisis_1_0299</td>\n",
       "      <td>[2.005553, -2.49519, 1.152471, -0.3920052, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s01a_11a_crisis_1_0300</td>\n",
       "      <td>[1.02881, -2.220682, 0.5428701, -0.5339947, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s01a_11a_crisis_1_0301</td>\n",
       "      <td>[0.4709102, -1.232904, 0.5122496, -0.4609921, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s01a_11a_crisis_1_0302</td>\n",
       "      <td>[1.015653, -1.081361, 0.232879, -0.3884549, 2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12809</th>\n",
       "      <td>s29h_9c_crisis_3_0299</td>\n",
       "      <td>[1.090773, -2.12289, 0.8505183, -0.4234217, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>s29h_9c_crisis_3_0300</td>\n",
       "      <td>[1.585508, -1.902868, 0.8690134, -0.3821687, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12811</th>\n",
       "      <td>s29h_9c_crisis_3_0301</td>\n",
       "      <td>[1.538649, -1.610638, 0.3587935, -0.5311078, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12812</th>\n",
       "      <td>s29h_9c_crisis_3_0302</td>\n",
       "      <td>[0.9480105, -1.187047, 0.9156968, -0.3451743, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12813</th>\n",
       "      <td>s29h_9c_crisis_3_0303</td>\n",
       "      <td>[0.5475618, -1.749468, 1.387854, -0.2542725, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12814 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         FILE  \\\n",
       "0      s01a_11a_crisis_1_0298   \n",
       "1      s01a_11a_crisis_1_0299   \n",
       "2      s01a_11a_crisis_1_0300   \n",
       "3      s01a_11a_crisis_1_0301   \n",
       "4      s01a_11a_crisis_1_0302   \n",
       "...                       ...   \n",
       "12809   s29h_9c_crisis_3_0299   \n",
       "12810   s29h_9c_crisis_3_0300   \n",
       "12811   s29h_9c_crisis_3_0301   \n",
       "12812   s29h_9c_crisis_3_0302   \n",
       "12813   s29h_9c_crisis_3_0303   \n",
       "\n",
       "                                                features  \n",
       "0      [0.5722328, -0.5864534, 0.4016994, -0.532431, ...  \n",
       "1      [2.005553, -2.49519, 1.152471, -0.3920052, 1.0...  \n",
       "2      [1.02881, -2.220682, 0.5428701, -0.5339947, 0....  \n",
       "3      [0.4709102, -1.232904, 0.5122496, -0.4609921, ...  \n",
       "4      [1.015653, -1.081361, 0.232879, -0.3884549, 2....  \n",
       "...                                                  ...  \n",
       "12809  [1.090773, -2.12289, 0.8505183, -0.4234217, -0...  \n",
       "12810  [1.585508, -1.902868, 0.8690134, -0.3821687, -...  \n",
       "12811  [1.538649, -1.610638, 0.3587935, -0.5311078, -...  \n",
       "12812  [0.9480105, -1.187047, 0.9156968, -0.3451743, ...  \n",
       "12813  [0.5475618, -1.749468, 1.387854, -0.2542725, -...  \n",
       "\n",
       "[12814 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30afbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.722328e-01, -5.864534e-01,  4.016994e-01, -5.324310e-01,\n",
       "        1.213925e+00, -4.061090e-01, -3.072732e-01, -7.375916e-01,\n",
       "        1.884446e+00,  4.039819e-01,  1.142112e+00, -1.813632e+00,\n",
       "       -1.358565e-01,  4.084460e-01,  8.180022e-03, -1.043652e+00,\n",
       "        1.789332e+00,  1.158190e+00, -1.128198e+00, -1.245530e+00,\n",
       "        6.612571e-01, -2.477030e-01,  2.289907e+00,  4.576121e-01,\n",
       "       -4.807441e-01,  9.407710e-01, -4.929501e-01,  6.233194e-01,\n",
       "        3.003048e-01, -2.441026e-01,  1.661693e+00,  1.086501e+00,\n",
       "        1.316072e+00, -2.465368e-03, -1.317744e+00, -6.016899e-01,\n",
       "        1.490715e+00,  8.050655e-01, -3.880918e-02,  1.958017e+00,\n",
       "        5.184799e-01,  4.137702e-01, -6.709865e-01, -6.479348e-01,\n",
       "       -1.311020e+00,  6.796974e-02, -2.334705e-01, -5.004630e-01,\n",
       "       -1.161196e+00, -8.000428e-01,  1.872293e+00,  3.460898e-01,\n",
       "        6.843317e-01,  1.680499e+00,  3.696207e-01, -1.553014e+00,\n",
       "       -5.100275e-01, -4.894818e-01,  1.283841e+00,  1.053119e+00,\n",
       "       -2.400882e-01, -1.232979e+00, -6.934044e-01, -1.330305e+00,\n",
       "       -2.641169e-01,  1.560446e+00, -8.586802e-01,  4.422697e-01,\n",
       "        6.108913e-01,  1.018693e+00, -9.000915e-01,  2.065496e-01,\n",
       "       -3.013883e-01, -6.516063e-01, -6.301992e-01, -4.167112e-01,\n",
       "        1.174617e+00, -3.777855e-01,  2.042475e+00, -1.242126e+00,\n",
       "        1.448800e+00, -7.299731e-01, -8.617753e-02, -3.825865e-02,\n",
       "       -3.668598e-01, -1.469409e+00,  1.303794e+00, -4.718100e-01,\n",
       "       -1.335134e+00,  6.922851e-01, -2.626909e-01,  1.117548e+00,\n",
       "        9.198101e-01,  4.368170e-01, -7.244407e-01, -5.417089e-01,\n",
       "        7.542359e-01,  1.468021e+00,  7.668973e-01,  1.398351e+00,\n",
       "       -1.431853e+00, -1.026667e+00,  1.612677e+00, -7.899830e-02,\n",
       "       -1.197803e+00,  2.720618e+00,  7.350364e-01, -7.389209e-01,\n",
       "        2.580126e-01,  9.828527e-01,  2.013796e+00,  6.581806e-01,\n",
       "       -4.505934e-01,  1.005324e+00,  6.905624e-01, -1.320107e-01,\n",
       "        1.079643e+00,  1.029081e+00,  1.436179e+00, -4.611138e-01,\n",
       "        2.738428e-01, -4.867419e-01, -8.719199e-01,  3.452727e-01,\n",
       "       -8.985260e-01,  1.096213e+00,  5.037331e-01,  1.529529e+00,\n",
       "       -6.406078e-01,  6.804433e-01,  4.459261e-01,  8.797433e-01,\n",
       "       -2.703017e-01,  1.647703e-01, -9.098666e-01, -5.730069e-01,\n",
       "       -3.627394e-01,  1.323610e+00, -6.603299e-01,  1.839041e+00,\n",
       "        1.507940e+00,  2.133180e+00,  2.926679e+00,  3.230434e-01,\n",
       "       -3.276033e-01,  1.583241e+00,  6.922770e-02, -8.939261e-02,\n",
       "       -9.302678e-01, -2.520256e+00,  2.518609e-01,  8.033926e-01,\n",
       "        1.869082e+00, -2.260506e-01, -1.084574e+00, -6.609377e-01,\n",
       "       -1.943673e+00, -2.387884e+00,  2.411004e+00,  1.295656e+00,\n",
       "       -3.600591e-01, -2.193173e+00,  2.507974e+00, -1.867387e+00,\n",
       "        1.068151e+00,  2.074383e+00,  1.682773e+00,  2.353813e+00,\n",
       "        1.377661e+00,  1.070281e+00,  1.160417e+00, -2.695095e-01,\n",
       "       -1.278654e+00, -2.544944e-01,  1.126718e+00,  8.260686e-01,\n",
       "       -7.271380e-01, -3.424502e-01,  2.268574e+00,  1.028151e-01,\n",
       "        1.880967e-01,  7.235097e-01,  9.638139e-01,  9.483762e-01,\n",
       "        1.186501e+00,  1.131601e+00, -1.901857e+00,  1.082189e+00,\n",
       "       -3.563259e-01, -1.406073e+00, -7.418921e-01,  8.616090e-02,\n",
       "        7.394890e-01, -8.949082e-01,  1.048228e+00,  2.895730e-01,\n",
       "       -2.493842e-01, -2.907965e-01, -6.293839e-01, -4.027991e-01,\n",
       "       -3.557380e-01,  1.147756e+00, -9.008508e-01, -1.256087e+00,\n",
       "        7.271934e-01,  1.610069e+00, -1.608900e+00, -9.818382e-01,\n",
       "       -6.514462e-01, -4.097546e-01, -1.096899e-01,  2.111048e+00,\n",
       "       -3.011461e-01,  1.561570e-01, -5.083525e-01, -9.635256e-01,\n",
       "        4.906557e-01,  1.433129e+00,  1.799396e+00,  7.465380e-01,\n",
       "       -5.043119e-01, -2.207474e-01, -5.608180e-01, -4.697721e-01,\n",
       "       -1.101877e+00, -1.483717e+00, -8.513720e-02,  8.495402e-01,\n",
       "        4.290237e-01, -6.647377e-01, -2.517728e-01,  1.113824e+00,\n",
       "       -1.195795e+00, -1.379227e+00,  2.485928e+00,  4.881571e-01,\n",
       "        9.273089e-01,  7.921137e-01, -3.554521e-01,  1.377464e+00,\n",
       "       -5.502590e-01,  1.036149e+00,  2.087574e+00,  1.210487e+00,\n",
       "        2.085375e+00,  1.097333e+00, -7.601655e-01,  1.531122e+00,\n",
       "       -1.434126e+00,  1.294295e+00, -2.020176e+00,  6.206930e-01,\n",
       "       -1.305897e+00,  2.347542e-01, -7.139406e-01,  1.149022e-01,\n",
       "       -7.358403e-01, -1.544015e+00, -1.544595e-01,  2.471719e+00,\n",
       "        1.035891e+00, -4.087385e-01, -5.850222e-01,  1.901106e+00,\n",
       "        1.530155e-01, -6.341173e-01, -1.339503e+00, -1.986870e+00,\n",
       "       -1.638303e-01,  6.329832e-01, -1.158524e+00, -4.583130e-02,\n",
       "       -6.674763e-01, -9.472560e-01,  6.626007e-01, -1.426785e+00,\n",
       "        1.000953e+00, -9.292932e-01, -3.486524e-01,  1.279981e+00,\n",
       "       -1.010653e+00, -1.787741e+00,  4.000942e-02,  7.857153e-01,\n",
       "       -1.290871e-01,  1.534688e+00, -7.220723e-01,  2.226532e+00,\n",
       "        1.030898e+00,  3.074937e-01,  8.815002e-01, -5.824338e-01,\n",
       "        9.204974e-01, -6.347247e-01, -1.679607e-01,  1.017450e+00,\n",
       "       -9.651585e-01, -4.696728e-01,  9.408168e-01, -8.524755e-01,\n",
       "        3.279235e-01, -1.780602e+00, -9.292706e-02,  7.867072e-01,\n",
       "       -8.535159e-01,  1.167710e+00, -3.961759e-01,  8.748220e-01,\n",
       "        7.740547e-01, -5.216600e-02, -7.689196e-01, -2.836204e-01,\n",
       "        1.213986e+00,  1.201781e+00,  2.181531e+00, -1.200670e+00,\n",
       "       -6.106973e-01,  1.870069e+00, -1.384483e-02,  1.190023e+00,\n",
       "        1.952763e+00, -1.020685e+00,  1.438793e+00,  1.423153e+00,\n",
       "        1.407844e+00,  1.141343e+00,  4.646983e-01, -6.362689e-01,\n",
       "        5.450248e-01, -1.152383e-01,  6.962683e-01,  8.197622e-01,\n",
       "        3.510602e-01,  6.983685e-01,  3.287040e-01,  1.230047e+00,\n",
       "       -2.495734e-01,  7.110462e-02,  2.345500e+00, -2.144856e+00,\n",
       "        6.058200e-01,  1.126530e+00, -1.845552e-01,  1.007652e+00,\n",
       "        2.726200e+00,  1.342592e-01, -9.089800e-01, -4.708005e-01,\n",
       "        8.662105e-01,  3.673912e-01, -8.678802e-01, -1.500794e-01,\n",
       "        1.251102e-01,  1.950833e+00,  7.959900e-02,  5.101993e-01,\n",
       "        1.118639e-02, -2.409839e-01,  1.978833e+00, -1.411445e+00,\n",
       "       -6.253846e-01,  9.930069e-01, -9.165615e-01, -1.403737e-01,\n",
       "        4.091386e-01,  9.221918e-01, -8.349290e-01, -7.882804e-01,\n",
       "        7.503445e-01,  1.061343e+00, -4.815140e-01, -3.964927e-01,\n",
       "        1.572236e+00,  2.079743e-01,  7.235551e-02, -2.008897e-01,\n",
       "       -1.289972e+00, -6.999202e-01,  9.966009e-01,  8.096074e-01,\n",
       "       -1.760965e+00,  9.317442e-01, -5.640156e-01,  1.966738e+00,\n",
       "        7.325442e-01, -1.804089e-01,  2.453480e+00,  2.282644e-01,\n",
       "       -6.369858e-01,  3.140040e-01, -1.197329e-01,  1.407586e+00,\n",
       "       -1.002611e+00, -1.057954e+00, -1.766239e+00, -1.348945e+00,\n",
       "        2.332284e-01,  9.543175e-01, -2.087274e-01, -6.129224e-01,\n",
       "       -9.688915e-01, -1.892774e+00,  6.088037e-01, -1.312564e+00,\n",
       "        7.325804e-01, -1.548790e+00, -7.859755e-01,  1.139765e+00,\n",
       "       -1.989208e+00,  9.193808e-01,  7.610618e-01,  5.961888e-02,\n",
       "        7.222665e-01, -4.241869e-01,  1.120227e+00,  2.167336e+00,\n",
       "       -5.986918e-01,  1.913451e+00,  1.247750e+00,  1.313655e+00,\n",
       "       -4.492595e-01, -3.839105e-01, -2.256870e-01,  1.379766e+00,\n",
       "       -6.686535e-01,  1.148883e+00, -8.487887e-01, -4.763174e-02,\n",
       "        2.433106e-02, -1.029243e+00, -1.758741e+00,  1.395767e+00,\n",
       "        9.325320e-01, -5.402893e-02, -9.515178e-02, -6.986147e-01,\n",
       "       -5.357016e-01,  2.009846e+00, -6.674736e-01, -1.481220e+00,\n",
       "        1.794315e+00,  9.659976e-01, -5.630272e-01, -1.329863e-01,\n",
       "        1.563369e+00,  1.227577e+00, -2.687766e-01, -4.990903e-01,\n",
       "        1.259570e+00, -3.645152e-01, -4.987706e-01,  4.578742e-01,\n",
       "       -2.410976e-01,  5.475817e-01, -8.184615e-01,  1.590144e+00,\n",
       "        2.657309e-01, -3.392358e-01,  1.191822e+00,  6.099312e-01,\n",
       "       -8.018360e-02,  8.200783e-02, -9.737192e-01, -6.067257e-01,\n",
       "        7.015948e-01, -5.469895e-01, -1.626917e-01, -5.377880e-01,\n",
       "       -3.576716e-01, -2.988323e-01,  1.133417e+00, -6.859532e-02,\n",
       "        9.952168e-01, -5.300210e-01, -3.573478e-03,  9.858305e-01,\n",
       "       -8.337407e-01, -6.528997e-01, -4.999698e-01, -2.247161e+00,\n",
       "       -4.217318e-01,  2.063301e-01,  9.531494e-01,  3.842380e-01,\n",
       "        2.290229e+00, -1.270973e+00, -1.003792e+00, -6.084117e-01,\n",
       "        7.508169e-01,  5.391293e-01,  8.564754e-01,  1.850335e+00,\n",
       "       -3.103506e-01,  1.723509e+00, -2.239427e-01,  4.353167e-01,\n",
       "       -6.690070e-01,  7.432919e-01,  1.628290e+00,  4.088285e-01,\n",
       "        3.842540e-01,  1.046781e+00,  6.889632e-01, -3.632013e-01,\n",
       "        2.422170e+00,  2.278861e-02, -1.689353e+00, -8.813763e-01,\n",
       "       -8.112519e-01, -6.229973e-01, -6.163396e-01,  2.147276e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['features'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c579f131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE</th>\n",
       "      <th>Annotators</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>SCALE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s06h_10b_crisis_2_0315</td>\n",
       "      <td>2</td>\n",
       "      <td>28.846021</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s01a_6b_crisis_2_0170</td>\n",
       "      <td>2</td>\n",
       "      <td>32.322045</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s07n_8a_crisis_1_0244</td>\n",
       "      <td>1</td>\n",
       "      <td>17.074338</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s06h_9b_crisis_2_0284</td>\n",
       "      <td>2</td>\n",
       "      <td>43.659132</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s07n_6a_crisis_1_0180</td>\n",
       "      <td>1</td>\n",
       "      <td>11.215002</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     FILE  Annotators      SCORE SCALE\n",
       "0  s06h_10b_crisis_2_0315           2  28.846021   med\n",
       "1   s01a_6b_crisis_2_0170           2  32.322045   med\n",
       "2   s07n_8a_crisis_1_0244           1  17.074338   low\n",
       "3   s06h_9b_crisis_2_0284           2  43.659132  high\n",
       "4   s07n_6a_crisis_1_0180           1  11.215002   low"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "audio_dataset_path=r'C:\\Users\\Jay Kejriwal\\Desktop\\Jay\\StressDat'\n",
    "header_list = [\"FILE\", \"Annotators\", \"SCORE\"]\n",
    "metadata=pd.read_csv(r'C:\\Users\\Jay Kejriwal\\Desktop\\Jay\\StressDat\\StressDat-anotacia-29spk_v1_normalize_all_scores.txt',sep=\"\\t\",names=header_list)\n",
    "conditions = [\n",
    "    (metadata['SCORE'] <= 20),\n",
    "    (metadata['SCORE'] > 20) & (metadata['SCORE'] <= 40),\n",
    "    (metadata['SCORE'] > 40)\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['low', 'med', 'high']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "metadata['SCALE'] = np.select(conditions, values)\n",
    "# display updated DataFrame\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb31405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE</th>\n",
       "      <th>Annotators</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>SCALE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s06h_10b_crisis_2_0315</td>\n",
       "      <td>2</td>\n",
       "      <td>28.846021</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s01a_6b_crisis_2_0170</td>\n",
       "      <td>2</td>\n",
       "      <td>32.322045</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s07n_8a_crisis_1_0244</td>\n",
       "      <td>1</td>\n",
       "      <td>17.074338</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s06h_9b_crisis_2_0284</td>\n",
       "      <td>2</td>\n",
       "      <td>43.659132</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s03a_9a_crisis_1_0274</td>\n",
       "      <td>1</td>\n",
       "      <td>27.351947</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     FILE  Annotators      SCORE   SCALE\n",
       "0  s06h_10b_crisis_2_0315           2  28.846021  Medium\n",
       "1   s01a_6b_crisis_2_0170           2  32.322045  Medium\n",
       "2   s07n_8a_crisis_1_0244           1  17.074338  Medium\n",
       "3   s06h_9b_crisis_2_0284           2  43.659132    High\n",
       "4   s03a_9a_crisis_1_0274           1  27.351947  Medium"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata=pd.read_csv(r'C:\\Users\\Jay Kejriwal\\Desktop\\Jay\\StressDat\\modified.csv',sep=\"\\t\")\n",
    "\n",
    "metadata.rename(columns={'V1': 'FILE', 'V2': 'Annotators', 'V3': 'SCORE'}, inplace=True)\n",
    "# display updated DataFrame\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0af94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X1= pd.merge(metadata, df, on='FILE', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f66dd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE</th>\n",
       "      <th>Annotators</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>SCALE</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s06h_10b_crisis_2_0315</td>\n",
       "      <td>2</td>\n",
       "      <td>28.846021</td>\n",
       "      <td>Medium</td>\n",
       "      <td>[1.384207, 0.6185268, -1.668165, -0.3523142, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s01a_6b_crisis_2_0170</td>\n",
       "      <td>2</td>\n",
       "      <td>32.322045</td>\n",
       "      <td>Medium</td>\n",
       "      <td>[0.8190287, -2.32593, -0.1740024, -0.3395131, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s07n_8a_crisis_1_0244</td>\n",
       "      <td>1</td>\n",
       "      <td>17.074338</td>\n",
       "      <td>Medium</td>\n",
       "      <td>[0.511696, 0.1088789, -1.544156, -0.6499531, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s06h_9b_crisis_2_0284</td>\n",
       "      <td>2</td>\n",
       "      <td>43.659132</td>\n",
       "      <td>High</td>\n",
       "      <td>[2.221089, 0.8119178, -1.537307, -0.5036674, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s03a_9a_crisis_1_0274</td>\n",
       "      <td>1</td>\n",
       "      <td>27.351947</td>\n",
       "      <td>Medium</td>\n",
       "      <td>[0.4488477, 0.7557455, -1.490509, -0.8269888, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12784</th>\n",
       "      <td>s29h_18a_crisis_1_0493</td>\n",
       "      <td>1</td>\n",
       "      <td>11.326830</td>\n",
       "      <td>Low</td>\n",
       "      <td>[0.6290638, -2.306679, 0.6345231, -0.312473, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12785</th>\n",
       "      <td>s27h_19a_crisis_1_0537</td>\n",
       "      <td>1</td>\n",
       "      <td>5.753345</td>\n",
       "      <td>Low</td>\n",
       "      <td>[1.2397, -1.037196, 0.4464797, -0.3504595, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12786</th>\n",
       "      <td>s26h_16b_crisis_2_0444</td>\n",
       "      <td>2</td>\n",
       "      <td>32.758280</td>\n",
       "      <td>Medium</td>\n",
       "      <td>[0.7678525, 1.456382, -0.9380628, -0.7516432, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12787</th>\n",
       "      <td>s25h_19a_crisis_1_0526</td>\n",
       "      <td>1</td>\n",
       "      <td>5.866519</td>\n",
       "      <td>Low</td>\n",
       "      <td>[1.230789, -0.1738276, -0.3944657, -0.2502984,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12788</th>\n",
       "      <td>s21h_16c_crisis_3_0472</td>\n",
       "      <td>3</td>\n",
       "      <td>50.199180</td>\n",
       "      <td>High</td>\n",
       "      <td>[0.7095287, -0.8968558, 0.5494157, -0.2304661,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12789 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         FILE  Annotators      SCORE   SCALE  \\\n",
       "0      s06h_10b_crisis_2_0315           2  28.846021  Medium   \n",
       "1       s01a_6b_crisis_2_0170           2  32.322045  Medium   \n",
       "2       s07n_8a_crisis_1_0244           1  17.074338  Medium   \n",
       "3       s06h_9b_crisis_2_0284           2  43.659132    High   \n",
       "4       s03a_9a_crisis_1_0274           1  27.351947  Medium   \n",
       "...                       ...         ...        ...     ...   \n",
       "12784  s29h_18a_crisis_1_0493           1  11.326830     Low   \n",
       "12785  s27h_19a_crisis_1_0537           1   5.753345     Low   \n",
       "12786  s26h_16b_crisis_2_0444           2  32.758280  Medium   \n",
       "12787  s25h_19a_crisis_1_0526           1   5.866519     Low   \n",
       "12788  s21h_16c_crisis_3_0472           3  50.199180    High   \n",
       "\n",
       "                                                features  \n",
       "0      [1.384207, 0.6185268, -1.668165, -0.3523142, 0...  \n",
       "1      [0.8190287, -2.32593, -0.1740024, -0.3395131, ...  \n",
       "2      [0.511696, 0.1088789, -1.544156, -0.6499531, 0...  \n",
       "3      [2.221089, 0.8119178, -1.537307, -0.5036674, 0...  \n",
       "4      [0.4488477, 0.7557455, -1.490509, -0.8269888, ...  \n",
       "...                                                  ...  \n",
       "12784  [0.6290638, -2.306679, 0.6345231, -0.312473, -...  \n",
       "12785  [1.2397, -1.037196, 0.4464797, -0.3504595, -0....  \n",
       "12786  [0.7678525, 1.456382, -0.9380628, -0.7516432, ...  \n",
       "12787  [1.230789, -0.1738276, -0.3944657, -0.2502984,...  \n",
       "12788  [0.7095287, -0.8968558, 0.5494157, -0.2304661,...  \n",
       "\n",
       "[12789 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff6d67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_df = X1[[\"SCALE\",\"features\"]]\n",
    "extracted_features_df.columns=['class_label','feature']\n",
    "extracted_features_df.to_pickle(\"newxvector_features512_1D.pkl\",protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c14226",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_df.to_csv(r\"D:\\j111.csv\", index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c342b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df['feature'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93068120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.384207  ,  0.6185268 , -1.668165  , -0.3523142 ,  0.8683042 ,\n",
       "       -1.108394  , -0.3603082 , -0.8947257 , -1.684454  , -1.1855    ,\n",
       "       -0.4131489 , -0.8488714 ,  0.4545313 ,  1.058568  ,  1.20606   ,\n",
       "        0.3362907 , -0.9347032 ,  0.2250484 , -1.098781  , -2.151877  ,\n",
       "        0.9154546 , -0.2313002 , -0.1204657 ,  0.6999055 , -1.681165  ,\n",
       "        0.5176618 ,  0.6816418 ,  2.448103  ,  0.6109436 ,  1.005803  ,\n",
       "        0.5958276 ,  0.5392126 ,  0.5262791 ,  1.25829   , -1.119233  ,\n",
       "       -0.9483609 , -2.03435   ,  1.153052  ,  1.16858   ,  0.0642302 ,\n",
       "       -2.223293  ,  1.443395  , -0.6524087 , -0.6485407 , -0.09221102,\n",
       "        0.03628117,  0.2939738 ,  0.1864458 ,  0.6499427 ,  0.7516132 ,\n",
       "        1.881889  , -2.362291  , -3.796108  , -1.074712  , -0.9332991 ,\n",
       "        0.3435362 , -0.7488071 , -0.679899  , -1.876489  ,  0.933697  ,\n",
       "        0.06523918,  0.6025363 ,  1.145786  ,  2.21687   , -0.560503  ,\n",
       "       -2.493037  ,  0.3860781 , -2.580408  , -0.5551303 , -1.002254  ,\n",
       "       -1.543518  , -0.8729404 , -0.3657712 , -0.2137984 ,  0.8598703 ,\n",
       "       -2.601948  ,  0.5272247 , -0.8516019 ,  0.3356461 ,  0.7889374 ,\n",
       "       -0.416151  , -0.8801873 , -0.3550134 , -1.966351  , -0.8900145 ,\n",
       "        1.632695  , -1.576428  , -0.1141075 ,  0.09446209, -1.406937  ,\n",
       "       -0.442798  ,  2.132025  ,  1.292222  ,  1.127767  ,  0.6834084 ,\n",
       "       -0.5088264 , -1.215811  ,  1.583938  , -0.4582387 , -0.6574042 ,\n",
       "       -0.6537346 ,  1.407905  ,  1.799074  ,  2.773911  ,  0.8280231 ,\n",
       "        2.220289  , -1.588574  , -0.4025582 , -3.195337  ,  2.444874  ,\n",
       "        0.09668617, -1.277422  , -0.765224  ,  1.726677  , -3.912639  ,\n",
       "       -0.2836962 , -1.498994  ,  2.087846  , -1.695929  ,  0.1678948 ,\n",
       "        1.098232  ,  0.6307918 ,  0.9400696 ,  0.7760831 ,  0.05476266,\n",
       "        2.473938  ,  0.0900321 ,  0.9938647 , -1.313514  ,  2.269762  ,\n",
       "        0.1843631 , -0.9759719 , -0.261927  , -1.595703  , -0.8130257 ,\n",
       "       -0.7922217 , -1.260324  , -0.05899036,  0.6948249 ,  0.3748552 ,\n",
       "        0.6575423 ,  1.22721   ,  1.425279  ,  1.963155  , -0.5973549 ,\n",
       "       -0.5255793 ,  0.4296588 ,  1.352992  , -0.7409251 ,  0.5566871 ,\n",
       "        0.8623971 , -1.676804  ,  2.354644  , -1.46817   , -0.03998578,\n",
       "       -0.7767532 ,  1.569098  ,  0.7978925 ,  1.268342  , -2.777785  ,\n",
       "        0.2093751 ,  1.140068  ,  0.6626498 , -1.221347  ,  0.7940111 ,\n",
       "        1.722879  ,  0.01386786,  1.516778  , -1.180586  ,  0.4025175 ,\n",
       "        0.6527369 ,  0.5542799 , -0.3307657 , -0.5102228 , -1.725349  ,\n",
       "        0.6112182 , -0.5199006 , -0.4028558 ,  1.679158  ,  0.2818742 ,\n",
       "       -2.128363  , -1.45487   ,  1.903531  ,  1.115754  ,  2.335561  ,\n",
       "       -1.645183  , -1.960782  ,  0.02182475, -1.254393  ,  0.8250935 ,\n",
       "        1.158693  ,  1.063472  ,  0.2137996 , -0.3909743 , -0.6999995 ,\n",
       "       -1.510799  ,  0.6478269 ,  1.048314  , -0.4758231 ,  0.4847527 ,\n",
       "       -0.09695118, -1.077671  ,  0.6798255 , -0.546787  ,  0.1490389 ,\n",
       "       -1.254178  , -3.53578   , -2.44343   , -0.304341  , -0.7275233 ,\n",
       "       -1.131612  ,  2.449617  , -0.835447  ,  0.1115089 , -2.933308  ,\n",
       "        1.476974  , -0.323739  , -1.573674  , -2.764358  , -1.558501  ,\n",
       "       -0.7088311 , -0.5446509 , -1.219842  , -0.8900071 ,  1.0223    ,\n",
       "        0.241353  , -0.6392351 , -2.061638  ,  2.030493  , -1.073898  ,\n",
       "       -0.4361161 , -0.1154283 , -0.5929589 ,  1.780825  , -2.121595  ,\n",
       "       -2.13109   ,  1.142208  , -0.6865113 , -0.2589902 ,  1.173206  ,\n",
       "        0.04165038,  1.102085  , -1.621347  , -0.4891044 ,  1.124281  ,\n",
       "        0.08629518, -0.227007  ,  0.04031118, -0.2553284 ,  1.54331   ,\n",
       "       -0.7574694 ,  0.6393314 ,  0.8347118 , -2.624645  ,  0.03467288,\n",
       "       -0.08353936, -0.7380215 ,  1.58633   , -0.3246984 ,  0.7917833 ,\n",
       "        0.167349  , -0.4881626 , -0.7043242 , -0.2556563 , -1.172073  ,\n",
       "       -0.8492025 , -1.522602  ,  1.479796  , -0.5997665 , -1.764318  ,\n",
       "        1.24589   , -1.98875   , -0.5648967 , -0.7588306 , -1.117934  ,\n",
       "        0.4582148 ,  1.006334  , -0.3132066 ,  1.509845  ,  0.8624294 ,\n",
       "        1.126555  , -0.04182312, -0.3934741 ,  1.727085  , -1.347493  ,\n",
       "        2.16943   , -0.8271556 ,  0.6903908 , -1.593942  , -1.084201  ,\n",
       "       -1.137905  ,  0.9795905 ,  1.052264  , -1.102125  , -1.443343  ,\n",
       "        0.9586754 , -0.1172267 ,  0.1374836 ,  1.056212  ,  0.5231999 ,\n",
       "       -1.332168  , -0.5277595 , -0.3891048 , -1.701835  ,  0.1989579 ,\n",
       "       -0.4953472 , -0.3124364 , -2.033408  , -3.904191  ,  0.8050451 ,\n",
       "       -0.3662301 , -0.575784  ,  3.144943  , -1.552376  ,  0.5040368 ,\n",
       "       -0.477953  , -0.7006776 , -0.1199841 ,  0.5890921 ,  1.087983  ,\n",
       "        0.869882  ,  0.8936872 ,  1.239855  ,  0.6011597 , -0.1885907 ,\n",
       "        0.4860665 , -1.713402  , -0.4912672 ,  1.147177  , -0.06392369,\n",
       "       -1.253454  , -1.336236  , -0.9620349 , -1.382376  ,  0.8174253 ,\n",
       "        0.4538085 , -1.320851  , -0.3286738 , -1.184458  ,  0.7623194 ,\n",
       "        1.39122   ,  1.072209  ,  0.2070197 , -0.968882  ,  0.5305344 ,\n",
       "       -1.002641  , -0.2612705 , -0.4359109 ,  1.326739  ,  1.037296  ,\n",
       "       -1.487839  ,  0.4732226 ,  0.2098287 ,  0.04030196, -0.8118587 ,\n",
       "       -0.6048559 ,  1.429682  , -0.8005086 ,  0.3671273 , -2.11038   ,\n",
       "       -0.9918514 ,  1.302458  , -0.7243039 , -0.3512909 , -2.186125  ,\n",
       "       -2.173348  , -0.446473  , -0.7653071 ,  1.482513  ,  1.506149  ,\n",
       "       -0.2841442 , -0.398206  , -3.020246  ,  0.9357994 ,  0.9663607 ,\n",
       "       -0.4802569 ,  1.431202  ,  1.273005  ,  2.623584  , -0.6420993 ,\n",
       "       -1.095584  ,  2.829793  , -1.122137  , -0.7612131 ,  0.5921285 ,\n",
       "       -2.078238  ,  2.792122  , -1.598124  , -0.3907014 ,  1.542933  ,\n",
       "       -0.3668885 , -1.560528  , -0.3629287 , -0.115606  ,  0.09055097,\n",
       "        0.3758005 ,  1.391866  , -1.614713  ,  1.54152   , -0.5516611 ,\n",
       "       -0.4755493 , -0.76742   ,  0.4282788 , -1.857813  , -1.273954  ,\n",
       "       -0.07909959, -1.022481  , -0.2139509 , -2.160345  , -0.2787777 ,\n",
       "       -0.2444224 ,  2.460712  ,  1.024861  , -1.585425  , -0.4958991 ,\n",
       "        0.455571  , -0.7476323 ,  2.644884  ,  1.299198  , -3.037026  ,\n",
       "        1.719783  , -1.160832  , -0.09382394, -1.393847  , -0.9226676 ,\n",
       "        0.2808151 ,  0.05813337, -1.328848  , -0.9938875 ,  0.6216319 ,\n",
       "        0.7903987 ,  1.576553  , -2.85429   , -0.3669124 , -1.656529  ,\n",
       "       -0.3000246 ,  0.1769277 , -0.02408677, -0.4289657 , -1.383455  ,\n",
       "        0.2165755 , -0.560762  , -0.5771916 ,  2.059694  , -0.6926191 ,\n",
       "        1.753093  , -0.3115824 , -0.6566657 , -0.6520864 , -0.406357  ,\n",
       "        0.5865759 , -1.186333  , -0.1539754 , -1.317121  ,  1.00299   ,\n",
       "        0.1108507 , -2.744931  , -0.5984598 ,  1.662883  ,  1.648243  ,\n",
       "        0.4446654 ,  0.5064515 , -1.817746  , -0.4831342 , -0.5891457 ,\n",
       "        0.65392   ,  1.90726   , -1.137122  ,  0.03084212,  0.903919  ,\n",
       "        1.840066  ,  0.6423252 ,  0.3999105 , -0.7560765 , -1.448524  ,\n",
       "       -0.236467  , -0.5999197 , -0.8987992 , -0.831771  ,  0.9119226 ,\n",
       "       -1.530276  , -0.692015  ,  0.2442961 , -1.077715  ,  2.596656  ,\n",
       "        0.8213283 ,  0.4177134 , -0.4245758 , -1.955101  , -0.5917593 ,\n",
       "        1.876737  ,  2.338527  , -0.451259  , -0.06059777, -2.133359  ,\n",
       "       -0.08035038, -0.9113755 , -0.8018486 ,  2.974123  , -2.476093  ,\n",
       "        0.8330473 ,  0.2004151 ,  0.7842044 , -0.3697006 ,  0.9451424 ,\n",
       "        2.311145  ,  0.1255074 , -2.938253  , -0.3170355 ,  0.4868624 ,\n",
       "        0.3780398 , -0.1520988 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df['feature'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be30dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293e8e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12789, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39bca221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12789,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7280f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Medium', 'Medium', 'Medium', ..., 'Medium', 'Low', 'High'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa86300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=np.array(extracted_features_df['class'].tolist())\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffdd96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c2aa966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f037b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e155e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10759145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(512,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b8cbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14962da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "320/320 [==============================] - 5s 11ms/step - loss: 1.0887 - accuracy: 0.4850 - val_loss: 0.6240 - val_accuracy: 0.7174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62397, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 2/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.6728 - val_loss: 0.6071 - val_accuracy: 0.7271\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62397 to 0.60708, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 3/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.6430 - accuracy: 0.6992 - val_loss: 0.5613 - val_accuracy: 0.7334\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.60708 to 0.56126, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 4/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6111 - accuracy: 0.7164 - val_loss: 0.5571 - val_accuracy: 0.7568\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56126 to 0.55715, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 5/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.5935 - accuracy: 0.7300 - val_loss: 0.5360 - val_accuracy: 0.7580\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55715 to 0.53596, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 6/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.5770 - accuracy: 0.7447 - val_loss: 0.5451 - val_accuracy: 0.7690\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53596\n",
      "Epoch 7/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.5715 - accuracy: 0.7418 - val_loss: 0.5363 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53596\n",
      "Epoch 8/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.5684 - accuracy: 0.7431 - val_loss: 0.5426 - val_accuracy: 0.7701\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53596\n",
      "Epoch 9/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5682 - accuracy: 0.7457 - val_loss: 0.5502 - val_accuracy: 0.7639\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53596\n",
      "Epoch 10/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5341 - accuracy: 0.7633 - val_loss: 0.5458 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.53596\n",
      "Epoch 11/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5316 - accuracy: 0.7615 - val_loss: 0.5289 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.53596 to 0.52891, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 12/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5353 - accuracy: 0.7625 - val_loss: 0.5429 - val_accuracy: 0.7694\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.52891\n",
      "Epoch 13/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5285 - accuracy: 0.7649 - val_loss: 0.5254 - val_accuracy: 0.7647\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.52891 to 0.52542, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 14/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5161 - accuracy: 0.7758 - val_loss: 0.5133 - val_accuracy: 0.7764\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.52542 to 0.51332, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 15/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5244 - accuracy: 0.7709 - val_loss: 0.5196 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51332\n",
      "Epoch 16/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.5285 - accuracy: 0.7675 - val_loss: 0.4959 - val_accuracy: 0.7772\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.51332 to 0.49594, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 17/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.5152 - accuracy: 0.7678 - val_loss: 0.5176 - val_accuracy: 0.7737\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49594\n",
      "Epoch 18/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5083 - accuracy: 0.7762 - val_loss: 0.5344 - val_accuracy: 0.7678\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49594\n",
      "Epoch 19/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.5208 - accuracy: 0.7761 - val_loss: 0.5072 - val_accuracy: 0.7838\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49594\n",
      "Epoch 20/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.5012 - accuracy: 0.7752 - val_loss: 0.5125 - val_accuracy: 0.7713\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49594\n",
      "Epoch 21/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4906 - accuracy: 0.7838 - val_loss: 0.5218 - val_accuracy: 0.7733\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.49594\n",
      "Epoch 22/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4999 - accuracy: 0.7824 - val_loss: 0.5041 - val_accuracy: 0.7737\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.49594\n",
      "Epoch 23/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4859 - accuracy: 0.7850 - val_loss: 0.5265 - val_accuracy: 0.7783\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49594\n",
      "Epoch 24/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4848 - accuracy: 0.7867 - val_loss: 0.5079 - val_accuracy: 0.7807\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.49594\n",
      "Epoch 25/100\n",
      "320/320 [==============================] - 1s 5ms/step - loss: 0.4873 - accuracy: 0.7856 - val_loss: 0.5150 - val_accuracy: 0.7823\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.49594\n",
      "Epoch 26/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4965 - accuracy: 0.7855 - val_loss: 0.5035 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49594\n",
      "Epoch 27/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4836 - accuracy: 0.7887 - val_loss: 0.5094 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49594\n",
      "Epoch 28/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4791 - accuracy: 0.7909 - val_loss: 0.5048 - val_accuracy: 0.7772\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.49594\n",
      "Epoch 29/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4936 - accuracy: 0.7795 - val_loss: 0.5197 - val_accuracy: 0.7783\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.49594\n",
      "Epoch 30/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4829 - accuracy: 0.7818 - val_loss: 0.5001 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.49594\n",
      "Epoch 31/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4770 - accuracy: 0.7933 - val_loss: 0.5215 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.49594\n",
      "Epoch 32/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4748 - accuracy: 0.7951 - val_loss: 0.4973 - val_accuracy: 0.7787\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.49594\n",
      "Epoch 33/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4716 - accuracy: 0.7905 - val_loss: 0.5021 - val_accuracy: 0.7807\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.49594\n",
      "Epoch 34/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4768 - accuracy: 0.7904 - val_loss: 0.5134 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.49594\n",
      "Epoch 35/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4580 - accuracy: 0.8022 - val_loss: 0.5021 - val_accuracy: 0.7842\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.49594\n",
      "Epoch 36/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4701 - accuracy: 0.7946 - val_loss: 0.5117 - val_accuracy: 0.7826\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.49594\n",
      "Epoch 37/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4675 - accuracy: 0.8000 - val_loss: 0.5280 - val_accuracy: 0.7713\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.49594\n",
      "Epoch 38/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4735 - accuracy: 0.7933 - val_loss: 0.5177 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.49594\n",
      "Epoch 39/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4661 - accuracy: 0.7979 - val_loss: 0.5017 - val_accuracy: 0.7834\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.49594\n",
      "Epoch 40/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4581 - accuracy: 0.8044 - val_loss: 0.5030 - val_accuracy: 0.7776\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.49594\n",
      "Epoch 41/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4803 - accuracy: 0.7936 - val_loss: 0.5213 - val_accuracy: 0.7729\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.49594\n",
      "Epoch 42/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4522 - accuracy: 0.8079 - val_loss: 0.5107 - val_accuracy: 0.7787\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.49594\n",
      "Epoch 43/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4605 - accuracy: 0.8015 - val_loss: 0.5144 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.49594\n",
      "Epoch 44/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4559 - accuracy: 0.8059 - val_loss: 0.5080 - val_accuracy: 0.7842\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.49594\n",
      "Epoch 45/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4470 - accuracy: 0.8078 - val_loss: 0.5151 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.49594\n",
      "Epoch 46/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4644 - accuracy: 0.7974 - val_loss: 0.5047 - val_accuracy: 0.7834\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.49594\n",
      "Epoch 47/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4510 - accuracy: 0.8006 - val_loss: 0.5158 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.49594\n",
      "Epoch 48/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4478 - accuracy: 0.8035 - val_loss: 0.5371 - val_accuracy: 0.7733\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.49594\n",
      "Epoch 49/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4629 - accuracy: 0.7933 - val_loss: 0.5042 - val_accuracy: 0.7885\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.49594\n",
      "Epoch 50/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4484 - accuracy: 0.8008 - val_loss: 0.5185 - val_accuracy: 0.7826\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.49594\n",
      "Epoch 51/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4470 - accuracy: 0.8093 - val_loss: 0.5089 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.49594\n",
      "Epoch 52/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4488 - accuracy: 0.8036 - val_loss: 0.5206 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.49594\n",
      "Epoch 53/100\n",
      "320/320 [==============================] - 2s 8ms/step - loss: 0.4468 - accuracy: 0.8068 - val_loss: 0.5135 - val_accuracy: 0.7807\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.49594\n",
      "Epoch 54/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4456 - accuracy: 0.8064 - val_loss: 0.5044 - val_accuracy: 0.7740\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.49594\n",
      "Epoch 55/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4494 - accuracy: 0.8024 - val_loss: 0.5016 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.49594\n",
      "Epoch 56/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4359 - accuracy: 0.8117 - val_loss: 0.5206 - val_accuracy: 0.7780\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.49594\n",
      "Epoch 57/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4482 - accuracy: 0.8090 - val_loss: 0.5025 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.49594\n",
      "Epoch 58/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4529 - accuracy: 0.8016 - val_loss: 0.5024 - val_accuracy: 0.7842\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.49594\n",
      "Epoch 59/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4483 - accuracy: 0.8153 - val_loss: 0.5039 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.49594\n",
      "Epoch 60/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4407 - accuracy: 0.8114 - val_loss: 0.5073 - val_accuracy: 0.7866\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.49594\n",
      "Epoch 61/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4459 - accuracy: 0.8065 - val_loss: 0.5064 - val_accuracy: 0.7881\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.49594\n",
      "Epoch 62/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4386 - accuracy: 0.8128 - val_loss: 0.5260 - val_accuracy: 0.7737\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.49594\n",
      "Epoch 63/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4472 - accuracy: 0.8056 - val_loss: 0.5298 - val_accuracy: 0.7815\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.49594\n",
      "Epoch 64/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4467 - accuracy: 0.8074 - val_loss: 0.5173 - val_accuracy: 0.7795\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.49594\n",
      "Epoch 65/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4412 - accuracy: 0.8057 - val_loss: 0.5139 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.49594\n",
      "Epoch 66/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4289 - accuracy: 0.8087 - val_loss: 0.5117 - val_accuracy: 0.7823\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.49594\n",
      "Epoch 67/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4237 - accuracy: 0.8176 - val_loss: 0.5075 - val_accuracy: 0.7889\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.49594\n",
      "Epoch 68/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4358 - accuracy: 0.8189 - val_loss: 0.5185 - val_accuracy: 0.7748\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.49594\n",
      "Epoch 69/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4381 - accuracy: 0.8113 - val_loss: 0.5064 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.49594\n",
      "Epoch 70/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4393 - accuracy: 0.8196 - val_loss: 0.5227 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.49594\n",
      "Epoch 71/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4335 - accuracy: 0.8117 - val_loss: 0.5055 - val_accuracy: 0.7756\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.49594\n",
      "Epoch 72/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4437 - accuracy: 0.8076 - val_loss: 0.5154 - val_accuracy: 0.7748\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.49594\n",
      "Epoch 73/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4413 - accuracy: 0.8116 - val_loss: 0.5129 - val_accuracy: 0.7776\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.49594\n",
      "Epoch 74/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4363 - accuracy: 0.8103 - val_loss: 0.4993 - val_accuracy: 0.7823\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.49594\n",
      "Epoch 75/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4400 - accuracy: 0.8084 - val_loss: 0.5101 - val_accuracy: 0.7815\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.49594\n",
      "Epoch 76/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4428 - accuracy: 0.8094 - val_loss: 0.5130 - val_accuracy: 0.7740\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.49594\n",
      "Epoch 77/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4378 - accuracy: 0.8094 - val_loss: 0.5129 - val_accuracy: 0.7807\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.49594\n",
      "Epoch 78/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4397 - accuracy: 0.8166 - val_loss: 0.5069 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.49594\n",
      "Epoch 79/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4276 - accuracy: 0.8162 - val_loss: 0.5221 - val_accuracy: 0.7838\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.49594\n",
      "Epoch 80/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4322 - accuracy: 0.8170 - val_loss: 0.5201 - val_accuracy: 0.7826\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.49594\n",
      "Epoch 81/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4300 - accuracy: 0.8168 - val_loss: 0.5048 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.49594\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4235 - accuracy: 0.8206 - val_loss: 0.5069 - val_accuracy: 0.7862\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.49594\n",
      "Epoch 83/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4271 - accuracy: 0.8136 - val_loss: 0.5370 - val_accuracy: 0.7721\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.49594\n",
      "Epoch 84/100\n",
      "320/320 [==============================] - 2s 8ms/step - loss: 0.4291 - accuracy: 0.8099 - val_loss: 0.5157 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.49594\n",
      "Epoch 85/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4163 - accuracy: 0.8235 - val_loss: 0.5308 - val_accuracy: 0.7826\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.49594\n",
      "Epoch 86/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4206 - accuracy: 0.8181 - val_loss: 0.5249 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.49594\n",
      "Epoch 87/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4245 - accuracy: 0.8169 - val_loss: 0.5506 - val_accuracy: 0.7658\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.49594\n",
      "Epoch 88/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4230 - accuracy: 0.8210 - val_loss: 0.5208 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.49594\n",
      "Epoch 89/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4299 - accuracy: 0.8173 - val_loss: 0.5148 - val_accuracy: 0.7795\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.49594\n",
      "Epoch 90/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4103 - accuracy: 0.8229 - val_loss: 0.5349 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.49594\n",
      "Epoch 91/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.4277 - accuracy: 0.8195 - val_loss: 0.5098 - val_accuracy: 0.7866\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.49594\n",
      "Epoch 92/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4046 - accuracy: 0.8262 - val_loss: 0.5314 - val_accuracy: 0.7862\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.49594\n",
      "Epoch 93/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.4168 - accuracy: 0.8194 - val_loss: 0.5033 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.49594\n",
      "Epoch 94/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4153 - accuracy: 0.8239 - val_loss: 0.5397 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.49594\n",
      "Epoch 95/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4258 - accuracy: 0.8160 - val_loss: 0.5143 - val_accuracy: 0.7866\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.49594\n",
      "Epoch 96/100\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.4145 - accuracy: 0.8230 - val_loss: 0.5188 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.49594\n",
      "Epoch 97/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4158 - accuracy: 0.8201 - val_loss: 0.5380 - val_accuracy: 0.7733\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.49594\n",
      "Epoch 98/100\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.4426 - accuracy: 0.8153 - val_loss: 0.5361 - val_accuracy: 0.7697\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.49594\n",
      "Epoch 99/100\n",
      "320/320 [==============================] - 2s 6ms/step - loss: 0.4215 - accuracy: 0.8274 - val_loss: 0.5485 - val_accuracy: 0.7697\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.49594\n",
      "Epoch 100/100\n",
      "320/320 [==============================] - 2s 5ms/step - loss: 0.4206 - accuracy: 0.8182 - val_loss: 0.5247 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.49594\n",
      "Training completed in time:  0:03:27.609738\n"
     ]
    }
   ],
   "source": [
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bab7df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8482064604759216\n",
      "0.784988284111023\n"
     ]
    }
   ],
   "source": [
    "train_accuracy=model.evaluate(X_train,y_train,verbose=0)\n",
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(train_accuracy[1])\n",
    "print(test_accuracy[1])\n",
    "#New dataset\n",
    "#0.8482064604759216\n",
    "#0.784988284111023\n",
    "\n",
    "#Old dataset\n",
    "#0.8511364459991455\n",
    "#0.7943815588951111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f207bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff9f7916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12789, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "645c1935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12789,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed5dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=np.array(extracted_features_df['class'].tolist())\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5478931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12789, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b160c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10231, 3) (2558, 3) (10231, 512, 1) (2558, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "### Train Test Split\n",
    "num_classes = 3;\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "Y_train_ann_cnn = y_train\n",
    "Y_test_ann_cnn = y_test\n",
    "\n",
    "print(Y_train_ann_cnn.shape,Y_test_ann_cnn.shape,X_train_cnn.shape,X_test_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97f0325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Convolution1D, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Convolution1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20f10bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 512, 16)           48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 256, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 256, 32)           1056      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 128, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 64, 128)           16512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 771       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,137,171\n",
      "Trainable params: 1,137,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=16, input_shape=X_train_cnn.shape[1:], kernel_size=2, activation='relu',padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=2, activation='relu',padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu',padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=2, activation='relu',padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3215721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5728b26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "320/320 [==============================] - 19s 52ms/step - loss: 1.0849 - accuracy: 0.3811 - val_loss: 0.7235 - val_accuracy: 0.6513\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72350, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 2/100\n",
      "320/320 [==============================] - 16s 50ms/step - loss: 0.7796 - accuracy: 0.6202 - val_loss: 0.6484 - val_accuracy: 0.7091\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72350 to 0.64838, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 3/100\n",
      "320/320 [==============================] - 16s 49ms/step - loss: 0.7132 - accuracy: 0.6705 - val_loss: 0.6076 - val_accuracy: 0.7232\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64838 to 0.60764, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 4/100\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.6819 - accuracy: 0.6757 - val_loss: 0.6042 - val_accuracy: 0.7256\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60764 to 0.60419, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 5/100\n",
      "320/320 [==============================] - 16s 50ms/step - loss: 0.6658 - accuracy: 0.6805 - val_loss: 0.5900 - val_accuracy: 0.7306\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.60419 to 0.59003, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 6/100\n",
      "320/320 [==============================] - 17s 54ms/step - loss: 0.6461 - accuracy: 0.7051 - val_loss: 0.5958 - val_accuracy: 0.7338\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59003\n",
      "Epoch 7/100\n",
      "320/320 [==============================] - 17s 54ms/step - loss: 0.6225 - accuracy: 0.7094 - val_loss: 0.5683 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.59003 to 0.56833, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 8/100\n",
      "320/320 [==============================] - 17s 54ms/step - loss: 0.6204 - accuracy: 0.7149 - val_loss: 0.5674 - val_accuracy: 0.7369\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.56833 to 0.56740, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 9/100\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.6151 - accuracy: 0.7153 - val_loss: 0.5779 - val_accuracy: 0.7314\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56740\n",
      "Epoch 10/100\n",
      "320/320 [==============================] - 17s 53ms/step - loss: 0.6034 - accuracy: 0.7213 - val_loss: 0.5526 - val_accuracy: 0.7572\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.56740 to 0.55257, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 11/100\n",
      "320/320 [==============================] - 16s 52ms/step - loss: 0.5817 - accuracy: 0.7359 - val_loss: 0.5568 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.55257\n",
      "Epoch 12/100\n",
      "320/320 [==============================] - 16s 51ms/step - loss: 0.5854 - accuracy: 0.7300 - val_loss: 0.5469 - val_accuracy: 0.7498\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.55257 to 0.54688, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 13/100\n",
      "320/320 [==============================] - 17s 54ms/step - loss: 0.5736 - accuracy: 0.7285 - val_loss: 0.5373 - val_accuracy: 0.7510\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.54688 to 0.53734, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 14/100\n",
      "320/320 [==============================] - 18s 55ms/step - loss: 0.5795 - accuracy: 0.7357 - val_loss: 0.5467 - val_accuracy: 0.7502\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.53734\n",
      "Epoch 15/100\n",
      "320/320 [==============================] - 16s 49ms/step - loss: 0.5691 - accuracy: 0.7355 - val_loss: 0.5490 - val_accuracy: 0.7553\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.53734\n",
      "Epoch 16/100\n",
      "320/320 [==============================] - 16s 49ms/step - loss: 0.5775 - accuracy: 0.7349 - val_loss: 0.5423 - val_accuracy: 0.7557\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.53734\n",
      "Epoch 17/100\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.5396 - accuracy: 0.7536 - val_loss: 0.5450 - val_accuracy: 0.7572\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.53734\n",
      "Epoch 18/100\n",
      "320/320 [==============================] - 16s 50ms/step - loss: 0.5607 - accuracy: 0.7517 - val_loss: 0.5300 - val_accuracy: 0.7545\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.53734 to 0.52996, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 19/100\n",
      "320/320 [==============================] - 16s 50ms/step - loss: 0.5495 - accuracy: 0.7552 - val_loss: 0.5273 - val_accuracy: 0.7639\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.52996 to 0.52728, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 20/100\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.5464 - accuracy: 0.7545 - val_loss: 0.5347 - val_accuracy: 0.7596\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.52728\n",
      "Epoch 21/100\n",
      "320/320 [==============================] - 16s 50ms/step - loss: 0.5376 - accuracy: 0.7556 - val_loss: 0.5317 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.52728\n",
      "Epoch 22/100\n",
      "320/320 [==============================] - 16s 51ms/step - loss: 0.5535 - accuracy: 0.7510 - val_loss: 0.5215 - val_accuracy: 0.7631\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.52728 to 0.52154, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 23/100\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.5438 - accuracy: 0.7584 - val_loss: 0.5422 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.52154\n",
      "Epoch 24/100\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.5262 - accuracy: 0.7569 - val_loss: 0.5206 - val_accuracy: 0.7654\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.52154 to 0.52055, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 25/100\n",
      "320/320 [==============================] - 16s 51ms/step - loss: 0.5372 - accuracy: 0.7585 - val_loss: 0.5146 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.52055 to 0.51461, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 26/100\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.5193 - accuracy: 0.7618 - val_loss: 0.5249 - val_accuracy: 0.7611\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51461\n",
      "Epoch 27/100\n",
      "320/320 [==============================] - 13s 40ms/step - loss: 0.5156 - accuracy: 0.7621 - val_loss: 0.5230 - val_accuracy: 0.7631\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51461\n",
      "Epoch 28/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.5257 - accuracy: 0.7603 - val_loss: 0.5269 - val_accuracy: 0.7658\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51461\n",
      "Epoch 29/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.5142 - accuracy: 0.7658 - val_loss: 0.5204 - val_accuracy: 0.7666\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51461\n",
      "Epoch 30/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.5176 - accuracy: 0.7721 - val_loss: 0.5268 - val_accuracy: 0.7678\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51461\n",
      "Epoch 31/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.5030 - accuracy: 0.7776 - val_loss: 0.5127 - val_accuracy: 0.7748\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.51461 to 0.51274, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 32/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4959 - accuracy: 0.7836 - val_loss: 0.5244 - val_accuracy: 0.7701\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51274\n",
      "Epoch 33/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.5072 - accuracy: 0.7748 - val_loss: 0.5117 - val_accuracy: 0.7690\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.51274 to 0.51174, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 34/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4796 - accuracy: 0.7851 - val_loss: 0.5215 - val_accuracy: 0.7682\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51174\n",
      "Epoch 35/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4831 - accuracy: 0.7868 - val_loss: 0.5185 - val_accuracy: 0.7705\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51174\n",
      "Epoch 36/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4845 - accuracy: 0.7914 - val_loss: 0.5197 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51174\n",
      "Epoch 37/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4969 - accuracy: 0.7813 - val_loss: 0.5251 - val_accuracy: 0.7631\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51174\n",
      "Epoch 38/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.4983 - accuracy: 0.7792 - val_loss: 0.5268 - val_accuracy: 0.7651\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51174\n",
      "Epoch 39/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.4988 - accuracy: 0.7835 - val_loss: 0.5123 - val_accuracy: 0.7694\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51174\n",
      "Epoch 40/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.4833 - accuracy: 0.7801 - val_loss: 0.5140 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51174\n",
      "Epoch 41/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.4734 - accuracy: 0.7888 - val_loss: 0.5276 - val_accuracy: 0.7686\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51174\n",
      "Epoch 42/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.4748 - accuracy: 0.7865 - val_loss: 0.5162 - val_accuracy: 0.7694\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51174\n",
      "Epoch 43/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4802 - accuracy: 0.7811 - val_loss: 0.5214 - val_accuracy: 0.7690\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51174\n",
      "Epoch 44/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4739 - accuracy: 0.7898 - val_loss: 0.5306 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51174\n",
      "Epoch 45/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.4532 - accuracy: 0.7979 - val_loss: 0.5268 - val_accuracy: 0.7654\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51174\n",
      "Epoch 46/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4711 - accuracy: 0.7799 - val_loss: 0.5246 - val_accuracy: 0.7686\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51174\n",
      "Epoch 47/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4512 - accuracy: 0.7943 - val_loss: 0.5249 - val_accuracy: 0.7666\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51174\n",
      "Epoch 48/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4737 - accuracy: 0.7869 - val_loss: 0.5198 - val_accuracy: 0.7717\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51174\n",
      "Epoch 49/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4564 - accuracy: 0.7966 - val_loss: 0.5243 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51174\n",
      "Epoch 50/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4637 - accuracy: 0.7934 - val_loss: 0.5173 - val_accuracy: 0.7678\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51174\n",
      "Epoch 51/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4615 - accuracy: 0.7966 - val_loss: 0.5299 - val_accuracy: 0.7666\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51174\n",
      "Epoch 52/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4509 - accuracy: 0.8014 - val_loss: 0.5158 - val_accuracy: 0.7705\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51174\n",
      "Epoch 53/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4405 - accuracy: 0.8010 - val_loss: 0.5178 - val_accuracy: 0.7787\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51174\n",
      "Epoch 54/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4449 - accuracy: 0.8042 - val_loss: 0.5363 - val_accuracy: 0.7682\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51174\n",
      "Epoch 55/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4708 - accuracy: 0.7976 - val_loss: 0.5252 - val_accuracy: 0.7682\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51174\n",
      "Epoch 56/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4353 - accuracy: 0.8089 - val_loss: 0.5288 - val_accuracy: 0.7678\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51174\n",
      "Epoch 57/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4268 - accuracy: 0.8085 - val_loss: 0.5199 - val_accuracy: 0.7768\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51174\n",
      "Epoch 58/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4472 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7697\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51174\n",
      "Epoch 59/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4375 - accuracy: 0.8088 - val_loss: 0.5393 - val_accuracy: 0.7783\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51174\n",
      "Epoch 60/100\n",
      "320/320 [==============================] - 10s 33ms/step - loss: 0.4397 - accuracy: 0.8077 - val_loss: 0.5317 - val_accuracy: 0.7690\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51174\n",
      "Epoch 61/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4350 - accuracy: 0.8076 - val_loss: 0.5233 - val_accuracy: 0.7705\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51174\n",
      "Epoch 62/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4211 - accuracy: 0.8175 - val_loss: 0.5295 - val_accuracy: 0.7701\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51174\n",
      "Epoch 63/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4399 - accuracy: 0.8069 - val_loss: 0.5346 - val_accuracy: 0.7686\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51174\n",
      "Epoch 64/100\n",
      "320/320 [==============================] - 10s 33ms/step - loss: 0.4279 - accuracy: 0.8129 - val_loss: 0.5295 - val_accuracy: 0.7740\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51174\n",
      "Epoch 65/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4209 - accuracy: 0.8137 - val_loss: 0.5356 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51174\n",
      "Epoch 66/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4297 - accuracy: 0.8170 - val_loss: 0.5213 - val_accuracy: 0.7701\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51174\n",
      "Epoch 67/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4279 - accuracy: 0.8073 - val_loss: 0.5171 - val_accuracy: 0.7705\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51174\n",
      "Epoch 68/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4182 - accuracy: 0.8196 - val_loss: 0.5249 - val_accuracy: 0.7639\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51174\n",
      "Epoch 69/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.4241 - accuracy: 0.8109 - val_loss: 0.5204 - val_accuracy: 0.7772\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51174\n",
      "Epoch 70/100\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.4206 - accuracy: 0.8128 - val_loss: 0.5220 - val_accuracy: 0.7651\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51174\n",
      "Epoch 71/100\n",
      "320/320 [==============================] - 12s 37ms/step - loss: 0.4202 - accuracy: 0.8165 - val_loss: 0.5282 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51174\n",
      "Epoch 72/100\n",
      "320/320 [==============================] - 12s 38ms/step - loss: 0.4064 - accuracy: 0.8246 - val_loss: 0.5338 - val_accuracy: 0.7686\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51174\n",
      "Epoch 73/100\n",
      "320/320 [==============================] - 13s 41ms/step - loss: 0.4067 - accuracy: 0.8235 - val_loss: 0.5340 - val_accuracy: 0.7686\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51174\n",
      "Epoch 74/100\n",
      "320/320 [==============================] - 12s 39ms/step - loss: 0.3996 - accuracy: 0.8216 - val_loss: 0.5406 - val_accuracy: 0.7608\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51174\n",
      "Epoch 75/100\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.3989 - accuracy: 0.8251 - val_loss: 0.5304 - val_accuracy: 0.7686\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51174\n",
      "Epoch 76/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3974 - accuracy: 0.8224 - val_loss: 0.5351 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51174\n",
      "Epoch 77/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3944 - accuracy: 0.8327 - val_loss: 0.5240 - val_accuracy: 0.7737\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51174\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3959 - accuracy: 0.8275 - val_loss: 0.5301 - val_accuracy: 0.7740\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51174\n",
      "Epoch 79/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3792 - accuracy: 0.8387 - val_loss: 0.5263 - val_accuracy: 0.7772\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51174\n",
      "Epoch 80/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.4008 - accuracy: 0.8273 - val_loss: 0.5422 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51174\n",
      "Epoch 81/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3813 - accuracy: 0.8397 - val_loss: 0.5327 - val_accuracy: 0.7654\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51174\n",
      "Epoch 82/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3828 - accuracy: 0.8351 - val_loss: 0.5249 - val_accuracy: 0.7787\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51174\n",
      "Epoch 83/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3901 - accuracy: 0.8306 - val_loss: 0.5367 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51174\n",
      "Epoch 84/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3964 - accuracy: 0.8284 - val_loss: 0.5393 - val_accuracy: 0.7721\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51174\n",
      "Epoch 85/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3943 - accuracy: 0.8280 - val_loss: 0.5375 - val_accuracy: 0.7686\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51174\n",
      "Epoch 86/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3810 - accuracy: 0.8368 - val_loss: 0.5318 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51174\n",
      "Epoch 87/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3841 - accuracy: 0.8357 - val_loss: 0.5383 - val_accuracy: 0.7733\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51174\n",
      "Epoch 88/100\n",
      "320/320 [==============================] - 10s 31ms/step - loss: 0.3920 - accuracy: 0.8309 - val_loss: 0.5311 - val_accuracy: 0.7690\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51174\n",
      "Epoch 89/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3836 - accuracy: 0.8332 - val_loss: 0.5551 - val_accuracy: 0.7694\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51174\n",
      "Epoch 90/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3805 - accuracy: 0.8356 - val_loss: 0.5288 - val_accuracy: 0.7772\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51174\n",
      "Epoch 91/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3696 - accuracy: 0.8360 - val_loss: 0.5345 - val_accuracy: 0.7764\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51174\n",
      "Epoch 92/100\n",
      "320/320 [==============================] - 10s 33ms/step - loss: 0.3843 - accuracy: 0.8336 - val_loss: 0.5400 - val_accuracy: 0.7705\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51174\n",
      "Epoch 93/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3934 - accuracy: 0.8313 - val_loss: 0.5324 - val_accuracy: 0.7701\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51174\n",
      "Epoch 94/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3676 - accuracy: 0.8457 - val_loss: 0.5528 - val_accuracy: 0.7690\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51174\n",
      "Epoch 95/100\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.3768 - accuracy: 0.8332 - val_loss: 0.5418 - val_accuracy: 0.7666\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51174\n",
      "Epoch 96/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3630 - accuracy: 0.8491 - val_loss: 0.5277 - val_accuracy: 0.7768\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51174\n",
      "Epoch 97/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3895 - accuracy: 0.8285 - val_loss: 0.5196 - val_accuracy: 0.7756\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51174\n",
      "Epoch 98/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3780 - accuracy: 0.8335 - val_loss: 0.5288 - val_accuracy: 0.7627\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51174\n",
      "Epoch 99/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3730 - accuracy: 0.8417 - val_loss: 0.5397 - val_accuracy: 0.7733\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51174\n",
      "Epoch 100/100\n",
      "320/320 [==============================] - 10s 32ms/step - loss: 0.3580 - accuracy: 0.8418 - val_loss: 0.5320 - val_accuracy: 0.7639\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51174\n",
      "Training completed in time:  0:20:03.938506\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train_cnn, Y_train_ann_cnn, batch_size=num_batch_size, epochs=num_epochs, \n",
    "                   validation_data=(X_test_cnn, Y_test_ann_cnn), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "\n",
    "#94\n",
    "#80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4aac3c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9451666474342346\n",
      "0.7638780474662781\n"
     ]
    }
   ],
   "source": [
    "train_accuracy=model.evaluate(X_train_cnn,Y_train_ann_cnn,verbose=0)\n",
    "test_accuracy=model.evaluate(X_test_cnn,Y_test_ann_cnn,verbose=0)\n",
    "print(train_accuracy[1])\n",
    "print(test_accuracy[1])\n",
    "\n",
    "#Accuracy on new dataset\n",
    "#0.9451666474342346\n",
    "#0.7638780474662781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55640155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras Tuner for auto tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da4e2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-503a2fbf775d>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "from kerastuner import RandomSearch\n",
    "import keras_tuner\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60d73988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):  \n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Conv1D(\n",
    "        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [2,5]),\n",
    "        activation='relu',\n",
    "        input_shape=X_train_cnn.shape[1:]\n",
    "    ),\n",
    "    keras.layers.Conv1D(\n",
    "        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values = [2,5]),\n",
    "        activation='relu'\n",
    "    ),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(\n",
    "        units=hp.Int('dense_1_units', min_value=32, max_value=256, step=16),\n",
    "        activation='relu'\n",
    "    ),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "  ])\n",
    "  \n",
    "  model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c364102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_search=RandomSearch(build_model,\n",
    "                          objective='val_accuracy',\n",
    "                          max_trials=5,directory='output',project_name=\"Stress_clasification_CNN4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "070268e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 49s]\n",
      "val_accuracy: 0.33365902304649353\n",
      "\n",
      "Best val_accuracy So Far: 0.7757694125175476\n",
      "Total elapsed time: 00h 04m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_search.search(X_train_cnn, Y_train_ann_cnn, epochs=3,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b44c9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tuner_search.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23b6f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 511, 64)           192       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 510, 64)           8256      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32640)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 176)               5744816   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 531       \n",
      "=================================================================\n",
      "Total params: 5,753,795\n",
      "Trainable params: 5,753,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35140c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "256/256 [==============================] - 16s 62ms/step - loss: 0.2614 - accuracy: 0.8905 - val_loss: 0.4939 - val_accuracy: 0.8075\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 16s 63ms/step - loss: 0.2126 - accuracy: 0.9142 - val_loss: 0.5425 - val_accuracy: 0.8012\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 16s 63ms/step - loss: 0.1697 - accuracy: 0.9337 - val_loss: 0.5122 - val_accuracy: 0.8085\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 16s 63ms/step - loss: 0.1358 - accuracy: 0.9460 - val_loss: 0.5969 - val_accuracy: 0.8095\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 16s 63ms/step - loss: 0.1037 - accuracy: 0.9616 - val_loss: 0.7073 - val_accuracy: 0.7885\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 16s 61ms/step - loss: 0.0828 - accuracy: 0.9688 - val_loss: 0.7587 - val_accuracy: 0.8065\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 16s 63ms/step - loss: 0.0458 - accuracy: 0.9856 - val_loss: 0.8468 - val_accuracy: 0.7987\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train_cnn, Y_train_ann_cnn, epochs=10, validation_split=0.2, initial_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60cd8912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9557228088378906\n",
      "0.7724785208702087\n"
     ]
    }
   ],
   "source": [
    "train_accuracy=model.evaluate(X_train_cnn,Y_train_ann_cnn,verbose=0)\n",
    "test_accuracy=model.evaluate(X_test_cnn,Y_test_ann_cnn,verbose=0)\n",
    "print(train_accuracy[1])\n",
    "print(test_accuracy[1])\n",
    "\n",
    "#0.9557228088378906\n",
    "#0.7724785208702087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06cf777f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABFiklEQVR4nO3dd3hc1bX38e/SaKRRry6yZVsy4IKLbFwwGIjBMRgIDoTQewJOJSQkENJJey83JISQEBJDTEmogdBCCSHYlxAgYBsDxgUXjCVX9d5GWu8f58xoJI1sWSNpVNbneeaZ02cflfObvfcpoqoYY4wxHcVEuwDGGGMGJgsIY4wxYVlAGGOMCcsCwhhjTFgWEMYYY8KygDDGGBOWBYQxw5yILBKRomiXwww8FhBmyBCR1SJSLiLx0S6LMUOBBYQZEkQkDzgRUGBZP392bH9+njH9xQLCDBWXA28C9wFXhM4QkXEi8jcRKRaRUhH5Xci8a0Rkk4hUi8hGETnGna4icmTIcveJyM/c4UUiUiQi3xaRfcC9IpIhIn93P6PcHc4NWT9TRO4VkT3u/Kfc6RtE5KyQ5bwiUiIis8PtpIh8SkTWi0iFiLwuIjPd6d8Wkcc7LPsbEbnDHb4qZD93iMgXevJDNsOLBYQZKi4HHnRfp4nIKAAR8QB/Bz4G8oCxwCPuvPOAm911U3FqHqXd/LzRQCYwAViO8790rzs+HqgHfhey/J+BRGAaMBL4tTv9AeDSkOXOAPaq6jsdP9ANjZXAF4As4I/AM26T2iPAGSKSErLf5wMPuasfAD7l7udVwK8DYWhMl1TVXvYa1C/gBKAZyHbHNwPfcIePA4qB2DDr/QO4rottKnBkyPh9wM/c4UVAE+A7SJlmAeXucA7QCmSEWW4MUA2kuuOPAzd2sc27gJ92mLYF+IQ7/BpwuTu8BNh+kPI9Fdh3d3+Kov17tNfAe1kNwgwFVwAvqWqJO/4Qbc1M44CPVdUfZr1xwPYefmaxqjYERkQkUUT+KCIfi0gV8CqQ7n6THweUqWp5x42o6h7gP8C5IpIOnI5TCwpnAvBNt3mpQkQq3G2Pcec/BFzkDl9MW+0BETldRN4UkTJ3vTOA7B7uuxkmrHPNDGoikoDTlOJx+wMA4nEOzgVAITBeRGLDhEQhcEQXm67DaRIKGA2Engra8TbI3wQmA8eq6j4RmQW8A4j7OZkikq6qFWE+637gapz/xzdUdXcXZSoEfq6qP+9i/l+BX7l9H+fg1J5wm6CewGlKe1pVm90+EOliO8YA1gdhBr+zgRbgaJxmnVnAVODfOAfEt4C9wC0ikiQiPhFZ6K57D/AtEZkjjiNFZII7bz1wsYh4RGQp8IlDlCMFp9+hQkQygR8FZqjqXuAF4PduZ7ZXRE4KWfcp4BjgOpw+ia7cDXxRRI51y5skImcG+h1UtRhYjdMX8pGqbnLXi8MJzWLALyKnA6ceYn+MsYAwg94VwL2quktV9wVeOB3El+B8Sz4LOBLYhVMLuABAVf8K/BynKaYa50Cd6W73One9Cnc7Tx2iHLcDCUAJztlUL3aYfxlOP8lmnA7jrwdmqGo9zjf8fOBvXX2Aqq4BrnH3rRzYBlzZYbGHgE8S0rykqtXA14DH3PUuBp45xP4Yg6jaA4OMiTYR+SEwSVUvPeTCxvQT64MwJsrcJqnP49QyjBkwrInJmCgSkWtwOp9fUNVXo10eY0JZE5MxxpiwrAZhjDEmrCHTB5Gdna15eXnRLoYxxgwqa9euLVHVEeHmDZmAyMvLY82aNdEuhjHGDCoi8nFX86yJyRhjTFgWEMYYY8IaMk1Mxhgz1JXVNvFRSQ3bi2v5qKSWHcU1fFRSy9j0BO69an6vf54FhDHGDCANzS3sLK3lo+JadpTUsqO4lh0lThBU1DUHl4uNEcZnJTIxO5nZ49P7pCwWEMYY089aW5U9lfVuLcCpCQTCYE9lPaGXp41KjSc/O4kzZuQwMTuJiSOSyM9OZlxGArGevu0lsIAwxpg+UlnXzPaSGrc2UBMMhI9Kamn0twaXS4rzMHFEMnMmZHDeiFzys5M4YkQyedlJJMdH7zBtAWGMMRFo9Lewq7SuU7/AjpJaymqbgst5YoTxmYlMzE7ihCOzmTgi2Q2CJEakxCMy8B7PYQFhjDGH0Nqq7KtqCAbAjpCaQFF5Ha0hTUIjUpwmodOmjSI/O4mJ2cnkj0hifGYi3j5uEuptFhDGGOOqamh2D/w1buewEwQ7S2qpb24JLpcY5yE/O4mZuWmcPXtssG8gLzuJVJ83invQuywgjDHDQpO/lcr6Zqoamqmsb6akupGPSmrbOopLaiipaWsSihEY5zYJHTcxi4kjktwgSGZU6sBsEuptFhDGmEFBValtaqGyvpnKurYDfVW9+97gbxsOTnPeK+ubaWhuDbvd7OQ48rOTWDxlFPnBEEhifGYScbGDq0mot1lAGGP6TXNLK1XuwTz0QB56MK+q93c6wAfWaWnt+vEEIpASH0tqgpc09zUxO5m0BC+pCbHuuzf4npkYR152EmkJQ6dJqLdZQBhjuk1VqW9uaTuo1/u7/MYeONCHTqtrajno9uM8Me5B3DnQZybFkZeVFDzgBw/0Pm+nA35KfCwxMUO/2ac/WUAYY9rxt7Syu6KenaV1fFxay84S9720lsLyepr84ZtqAgLf4gMH+vGZie0O5gc70Pu8nn7aS9MdFhDGDENN/lYKyzsGgPNeVF6PP6QpJ8HrYUJWIkeNTGHx1FFkJsW1O7iHHvCT42P7/Ope038sIIwZohqaW9hVVsfOklo+Lq1jZ2nb+56K+nbn7ifHx5KXnci0sWmcOTOHCVlJ5GUlkZeVOGAv4jJ9zwLCmEGsttHPx6XtawCBINhb2dBu2fRELxOykpgzIYPPHJNLXlaiGwSJZCbFWQiYTiwgjBngqhqa+bgkUAMIDYI6iqsb2y2bnRzHhKwkjjsii7ysJCZkJQbf0xPjorQHZrCygDAmylSVirrmdk1Aoe+h9/MB5+6eE7KSOHnyiGBT0ISsRCZkJZIyhK7iNdFnAWFMP1BVSmqaOtUAnE7iWqoa/MFlRWBMWgITshI5bdrotqag7ETGZyaSGGf/tqZ/2F+aMb2grsnP3soG9lY0sLeynn2VDeypbGBfZT17KxsoLKujNuQaAE+MMDbdCYFPzxobbArKy04kNyPRTvc0A4IFhDGHUNfkZ09FA/sqnYP/3soG91XvTmugsr6503pZSXGMTvORm5HAgolZTk0g22kSys1IGHR39jTDjwWEGdZqG/3Bg/3eyg4h4NYGQpt/ArKTAwf/RObnZzI6zceYtARGp/nISfMxKtVntQAz6FlAmCGrptEfbOJxDvYN7KuqD9YG9lTWU93FwT8nLYHxWYkcOzGTnLQEctJ8wRAYmRpvB38zLFhAmEEpcPAPPdjvC2n62VvZ0MXBP56cNB8TshJZMDGT0WkJjEn3MTrVx5h05+AfH2sHf2PAAsIMQK2tSlF5PTtLa9s391S5nb4VDVQ3hj/4j0n3kZeVxPFHZAebewI1ADv4G3N4LCBMVJXUNPLhvmo276tmy75qtuyv5sP91e3u+iniHvzTfORnOwf/QJNP4OA/KtU37O/db0xviyggROQs4DlVPfjtHc2wV9voZ+uBGrbsq2LzPicEtuyrbvcEr8ykOCaPSuH8ueOYMjqFiSOS7eBvTBRFWoO4ALhdRJ4AVqrq5l4okxnEmlta2VlSGwyBQM1gV1ldcJkEr4dJo5I5ZcpIJo1KYcroVCaPTiE72e4HZMxAElFAqOqlIpIKXATcJyIK3As8rKrVvVFAMzCpKnsqG9iyr4ot+9pqBjuKa2lqcSqUnhghPzuJGWPT+OycXCaPTmHK6BTGZSTag12MGQQi7oNQ1SoReRxIAL4OnAPcICJ3qOpvI92+ib6KuqZg/8DmfdV86A6HniWUk+Zj8ugUPjF5BFNGpzBpVApHjEi200GNGcQi7YNYBlwFHAk8AMxX1QMikghsBCwgBpGG5ha2Hahxm4Wq2LLfqRnsr2q7Y2iqL5Ypo1M5e9ZYJrk1gkmjUuy5vsYMQZHWIM4Ffq2qr4ZOVNU6Efl8hNs2faSlVfm4tDZYK9ji9hPsLK0NPkQmLjaGo0Yms/CIbCaPTnGbh1IZlWoPjzFmuIg0IG4G9gZGRCQBGKWqO1X1XxFu20RIVTlQ3RgMgEDH8dYD1TQ0O/0EIpCXlcSkUcl8qmBMsEaQl5Voj440ZpiLNCD+ChwfMt7iTpsX4XbNYWpuaWXz3mre210RDIQt+6upqGu7idyIlHimjE7h0mMnBGsFR41MISHO+gmMMZ1FGhCxqho8kV1Vm0TEHlvVx1SVwrJ63iks593CStYXlvPBnioa/U6tIDk+lkmjkjl9eg6TRyUz2T2NNDPJfjXGmO6LNCCKRWSZqj4DICKfBkoiL5YJVVHXxPrCCtYXVvBuYQXvFlUGnzLm88YwY2waly2YwKzx6RTkppObkWD9BMaYiEUaEF8EHhSR3wECFAKXR1yqYazR38LGPVXBMFhfWMHOUuciMxE4amQyi6eMDIbB5NEp9lwBY0yfiPRCue3AAhFJdsdreqVUw0Rrq/JRaW0wCN4trGDj3iqaW5xTiUalxjNrXDrnzxvHrHHpzBibZs8cNsb0m4gvlBORM4FpgC/QrKGqPznEOkuB3wAe4B5VvaXD/AnASmAEUAZcqqpFkZY12kpqGlm/q4J3i9oCIfAwmqQ4DzNy0/j8CROZNS6NWeMyGJ3mi3KJjTHDWaQXyv0BSAROBu4BPgu8dYh1PMCdwBKgCHhbRJ5R1Y0hi/0SeEBV7xeRU4D/AS6LpKz9rb6phQ17Knm3sIJ3CitYv6uC3RX1gHMLikmjUjhz5phgGBw5MhmP3X7CGDOARFqDOF5VZ4rIe6r6YxH5FfDCIdaZD2xT1R0AIvII8GmcK68Djgaud4dXAU9FWM4+1dKqbC+uYf2uCtYXOWGwZX81Le5VZ2PTE5g1Lp0rj8+jYFw608emkhhnd1o3xgxskR6lGtz3OhEZA5QCOYdYZyxOZ3ZAEXBsh2XeBT6D0wx1DpAiIlmqWhq6kIgsB5YDjB8/vkc70BP7KhvanVX0/u5KatwH2KT4YinITedLnziCWePSmTkujZEp1lRkjBl8Ig2IZ0UkHbgVWAcocHekhQK+BfxORK4EXgV241yE146qrgBWAMydO1d74XM7qWn0835RZbuzivZVObno9QhTc1I5Z/ZYZo1Lp2BcOhOzk+xOpcaYIaHHASEiMcC/VLUCeEJE/g74VLXyEKvuBsaFjOe604JUdQ9ODQL3DKlz3c/pU/6WVrbsrw5efPZuYSUfHqhG3eiZ4D7EviA3nVnj0zk6J9XuVmqMGbJ6HBCq2ioidwKz3fFGoPHgawHwNnCUiOTjBMOFwMWhC4hINlDmPqnuOzhnNPWJstom7lq9jfVuU1HgHkUZiV4KxqWzdPro4DUHdiWyMWY4ibSJ6V8ici7wN1XtVhOPqvpF5KvAP3BOc12pqh+IyE+ANe5V2YuA/3EfQPQq8JUIy9ml+NgY/vLmLqbkpHDR/PHMGpfOrHHpjM9MtKuRjTHDmnTzuB5+ZZFqIAnw43RYC6Cqmto7xeu+uXPn6po1a3q0rr+l1e5caowZlkRkrarODTcv0iupUyJZf6CwcDDGmM4ivVDupHDTOz5AyBhjzOATaR/EDSHDPpyL4NYCp0S4XWOMMVEWaRPTWaHjIjIOuD2SbRpjjBkYervxvQiY2svbNMYYEwWR9kH8FufqaXDCZhbOFdXGGGMGuUj7IELPK/UDD6vqfyLcpjHGmAEg0oB4HGhQ1RZwbuUtIomqWhd50YwxxkRTpH0Q/wISQsYTgJcj3KYxxpgBINKA8IU+ZtQdToxwm8YYYwaASAOiVkSOCYyIyBygPsJtGmOMGQAi7YP4OvBXEdmDcx+m0cAFkRbKGDN4NDc3U1RURENDw6EXNlHj8/nIzc3F6/V2e51IL5R7W0SmAJPdSVtUtTmSbRpjBpeioiJSUlLIy8uzOyAPUKpKaWkpRUVF5Ofnd3u9iJqYROQrQJKqblDVDUCyiHw5km0aYwaXhoYGsrKyLBwGMBEhKyvrsGt5kfZBXBP6pDdVLQeuiXCbxphBxsJh4OvJ7yjSgPBIyKeKiAewx64ZY8wQEGlAvAg8KiKLRWQx8DDwQuTFMsaY7qmoqOD3v/99j9Y944wzqKio6N0CDSGRBsS3gVeAL7qv92l/4ZwxxvSpgwWE3+8/6LrPP/886enpfVCqyKgqra2t0S5GZAGhqq3Af4GdOM+COAXYFHmxjDGme2666Sa2b9/OrFmzuOGGG1i9ejUnnngiy5Yt4+ijjwbg7LPPZs6cOUybNo0VK1YE183Ly6OkpISdO3cydepUrrnmGqZNm8app55KfX3nS7qeffZZjj32WGbPns0nP/lJ9u/fD0BNTQ1XXXUVM2bMYObMmTzxxBMAvPjiixxzzDEUFBSwePFiAG6++WZ++ctfBrc5ffp0du7cyc6dO5k8eTKXX34506dPp7CwkC996UvMnTuXadOm8aMf/Si4zttvv83xxx9PQUEB8+fPp7q6mpNOOon169cHlznhhBN49913I/rZ9ug0VxGZBFzkvkqARwFU9eSISmOMGdR+/OwHbNxT1avbPHpMKj86a1qX82+55RY2bNgQPDiuXr2adevWsWHDhuApnStXriQzM5P6+nrmzZvHueeeS1ZWVrvtbN26lYcffpi7776b888/nyeeeIJLL7203TInnHACb775JiLCPffcwy9+8Qt+9atf8dOf/pS0tDTef/99AMrLyykuLuaaa67h1VdfJT8/n7KyskPu69atW7n//vtZsGABAD//+c/JzMykpaWFxYsX89577zFlyhQuuOACHn30UebNm0dVVRUJCQl8/vOf57777uP222/nww8/pKGhgYKCgm7/nMPp6XUQm4F/A59S1W0AIvKNiEpijDG9ZP78+e3O97/jjjt48sknASgsLGTr1q2dAiI/P59Zs2YBMGfOHHbu3Nlpu0VFRVxwwQXs3buXpqam4Ge8/PLLPPLII8HlMjIyePbZZznppJOCy2RmZh6y3BMmTAiGA8Bjjz3GihUr8Pv97N27l40bNyIi5OTkMG/ePABSU1MBOO+88/jpT3/KrbfeysqVK7nyyisP+XmH0tOA+AxwIbBKRF4EHsG5ktoYM4wd7Jt+f0pKSgoOr169mpdffpk33niDxMREFi1aFPZ6gPj4+OCwx+MJ28R07bXXcv3117Ns2TJWr17NzTfffNhli42Nbde/EFqW0HJ/9NFH/PKXv+Ttt98mIyODK6+88qDXMSQmJrJkyRKefvppHnvsMdauXXvYZeuoR30QqvqUql4ITAFW4dxyY6SI3CUip0ZcKmOM6aaUlBSqq6u7nF9ZWUlGRgaJiYls3ryZN998s8efVVlZydixYwG4//77g9OXLFnCnXfeGRwvLy9nwYIFvPrqq3z00UcAwSamvLw81q1znqu2bt264PyOqqqqSEpKIi0tjf379/PCC84JopMnT2bv3r28/fbbAFRXVwc746+++mq+9rWvMW/ePDIyMnq8nwGRdlLXqupD7rOpc4F3cM5sMsaYfpGVlcXChQuZPn06N9xwQ6f5S5cuxe/3M3XqVG666aZ2TTiH6+abb+a8885jzpw5ZGdnB6d///vfp7y8nOnTp1NQUMCqVasYMWIEK1as4DOf+QwFBQVccIFzm7pzzz2XsrIypk2bxu9+9zsmTZoU9rMKCgqYPXs2U6ZM4eKLL2bhwoUAxMXF8eijj3LttddSUFDAkiVLgjWLOXPmkJqaylVXXdXjfQwlqnropQaBuXPn6po1aw69oDGmV23atImpU+1R9APBnj17WLRoEZs3byYmpvP3/3C/KxFZq6pzw20v0usgjDHGDAAPPPAAxx57LD//+c/DhkNPRHq7b2OMMQPA5ZdfzuWXX96r27QahDHGmLAsIIwxxoRlAWGMMSYsCwhjjDFhWUAYYwa1SG73DXD77bdTV1fXiyUaOiwgjDGD2lAIiEPdljxaLCCMMYNax9t9A9x6663MmzePmTNnBm+TXVtby5lnnklBQQHTp0/n0Ucf5Y477mDPnj2cfPLJnHxy55tR/+QnP2HevHlMnz6d5cuXE7iweNu2bXzyk5+koKCAY445hu3btwPwv//7v8yYMYOCggJuuukmABYtWkTgIt6SkhLy8vIAuO+++1i2bBmnnHIKixcvpqamhsWLF3PMMccwY8YMnn766WA5HnjgAWbOnElBQQGXXXYZ1dXV5Ofn09zcDDi35Qgd7y12HYQZWFpboKES6suhuQ60te3V2tp+XFs6jKuzfthl1N1GX8zXkDJ2MT8lB0bPgJyZkJ4HvXQh04Dzwk2w7/3e3eboGXD6LV3O7ni775deeomtW7fy1ltvoaosW7aMV199leLiYsaMGcNzzz0HOPdVSktL47bbbmPVqlXtbp0R8NWvfpUf/vCHAFx22WX8/e9/56yzzuKSSy7hpptu4pxzzqGhoYHW1lZeeOEFnn76af773/+SmJjYrdt7r1u3jvfee4/MzEz8fj9PPvkkqamplJSUsGDBApYtW8bGjRv52c9+xuuvv052djZlZWWkpKSwaNEinnvuOc4++2weeeQRPvOZz+D1envwA+6aBURjNTx8EcSnQnwK+FLDDKe6wyltw3EpQ/efPFItzVBfAQ0Vnd87TasMmVcJjb37LIE+IR6QmLZXTGBc3PcO80Wgep8TFuD87YyeDqNnOge/0TNg5FSIjT/455pueemll3jppZeYPXs24DzMZ+vWrZx44ol885vf5Ns33sinTj+VE4+bD021gEKr3wlyaX9T6lWrVvGLX/yCurq64P2TFi1axO7duznnnHMA8Pl8gHPL76uuuorExESge7f3XrJkSXA5VeW73/0ur776KjExMezevZv9+/fzyiuvcN555wUDLLD81VdfzS9+8QvOPvts7r33Xu6+++7If3gdRCUgRGQp8BvAA9yjqrd0mD8euB9Id5e5SVWf75PC+JucP47ynU5YNFY679qNx/3FpRwkVFLAl9Y+VILDIdNj4zv9UQ4IzQ1dHOAruz74Bw74zbUH33ZsAiSkgy/deU/LdQ6YvrS2ab50iEt0DrYxnvYH2y4P0P00v6c/zwMbnW/X+96Hfe/BO39p+1nFxMKIKW5gBIJjOiREfkfOfnWQb/r9RVX5zne+wxeuuRr8jeBvCL7WvfgQz//zFb7/3W+z+IT5/PAby50vNAc2gn+v+/cWCzEeGpr8fPlLX2DNqucYN248N99yGw3V5c7fOOr8TmPc5Q/ydxF6e++Ot+sOvb33gw8+SHFxMWvXrsXr9ZKXl3fQ23svXLiQnTt3snr1alpaWpg+fXpkP7hwZe/1LR6CiHiAO4ElQBHwtog8o6obQxb7PvCYqt4lIkcDzwN5fVKgpCz43Ivtp6k63ywaq5ywaKhyh6vc4eoO89xQqS+Hio/bpvs730++kxhvN0MlpDbTMYjiU5w/1I770Fx3kG/yBznQN1Q6/1AHE5fc/mCeOTFkvMOBvt172vD8puz1wdhjnFdAayuUfwR7320Lju2vwLsPty2TPj4kMNz3tNyB+aUiSlKSEqiuroLaUvA3cNpxM/jB/7uNS06ZTnJSIrv3HsDrjcWPh8zsUVx6xedIH3ME99z/IGROJCU1nWpSyE4e7XxZbG0B9dNQXwWqZCcINft38PjfnuSzZy4mpbmY3JGZPPXn33P20pNpbGyipRWWzJ3ET277PZectoDE5GTKKmvIzMoib+wo1r7xf8yfMYnHH3kQUCeUOnwJraysZOTIkXi9XlatWsXHH38MwCmnnMI555zD9ddfT1ZWFmVlZcFaxOWXX87FF1/MD37wgz752UajBjEf2KaqOwBE5BHg00BoQCiQ6g6nAXv6tYQiEJ/svCLR0uyGRWWYUAkTOIHhil3tlzmc2ozX56zbUAmth+iwik+DhJCD+YjJnQ/mweGMtnm+NPBY62TEYmIg6wjnNf0zbdOr98P+92Hve23Bsfk5nH8LnFpFu5rGTMg+Cjy92/484LQ0h9QGGqG5HvyNZLU2s/CYaUyfcyynn7yQW3/8HTZtPYfjzr4aREhOTuEvf/kL27bv4IZLLyAmJgav18tdd90FvjSWf/FLLD33UsaMGcOqVauCH5eeBdd84UtMX3IRo0ePZt6ChZA0ArIn8ecHHuALX/kaP7ztHrzeWP56310sXbqU9Zu2MXfJOcR5YznjlBP4fzd9hW99/lzO/+K3WXH3nzhz8QnOfuzfABWFUFsC+zZATCyXLF3AWZfez4yjpzD3mAKmTDoK6sqYNnEi37vxej5x0ol4PLHMnj2b++67D0S45JJL+P73v89FF13UJz/yfr/dt4h8Fliqqle745cBx6rqV0OWyQFeAjKAJOCTqtrp8UgishxYDjB+/Pg5gcQdUgI1gWBgdAicdjWaKuefJj61/cE83Df5+NTOtQ4zcDXWuE1U77UFx4GNbTU9T7zTj5Ezsy04Rk2P/EtON/Tq7b5VOwSB+2puaOvDAae5L9bX/uWNd34OA6l2FTy5wa2ZtPqd/QiOt7SrtbSbxsGOzQIxHh5/7l88/Y9V/PnuO52a5SEc7u2+B+rXwIuA+1T1VyJyHPBnEZmu2v6rtKquAFaA8zyIKJSz74lAXJLzIifapTHREp8M4+Y7r4AWP5RubevT2PsebHoW1j3gLiBO01/g7KlAcKSMjsoutKMKLU3tAyAwHPpvLh7n4J+Q3j4MPN6BFQRdCfRpHe6XsXZnzbW0Dxg3TK791vd44eXVPP/wiu61MvRANAJiNzAuZDzXnRbq88BSAFV9Q0R8QDZwoF9KaMxg4Il1ag0jp8LM851pqlC1xwmMQHDseQc2PtW2XtLItrOnRs+AnAInSPqiRqmtzokg4YIg9BtyTKwbBJlOM2kgCA7RATxkdSNYfrvivj4vRjQC4m3gKBHJxwmGC4GLOyyzC1gM3CciUwEfUNyvpTRmMBKBtLHOa/LpbdPrK2D/B22hse89eOPOtn4qbyKMmta+X2PU0eBN6NbHaksL0troBkCjc4KGv9F5hQaBJ845SSF+hPMe63MCIWagNmYMHT3pTuj334qq+kXkq8A/cE5hXamqH4jIT4A1qvoM8E3gbhH5Bs5f15U6VJ6Nakw0JKRD3kLnFeBvguLN7U+9ff+vsOZPznyJgexJ7TvEsyc513SUbHHWLd6Cb9TplHqqyUqKRQLf9j3uwd+X5tYG3HHr94oKVaW0tDR4zUZ32TOpjTFtVJ1TtUPPoNr3HlR1bAXGOUU760iaR8+iaPy5NMRlODWE4dosNMD5fD5yc3M7XW09GDupjTHRIAIZec7r6GVt02tLnVNvS7Y6tw0ZMRky8sETixfIj1JxTd+ygDDGHFpSFkxc5LzMsGE3EzLGGBOWBYQxxpiwhkwntYgUA5FcSp0NlPRScaJpqOwH2L4MVENlX4bKfkBk+zJBVUeEmzFkAiJSIrKmq578wWSo7AfYvgxUQ2Vfhsp+QN/tizUxGWOMCcsCwhhjTFgWEG1WRLsAvWSo7AfYvgxUQ2Vfhsp+QB/ti/VBGDPEiMhq4C+qek+0y2IGN6tBmGFJRHaKSL2I1IS8fhftchkzkNiV1GY4O0tVXz7UQiISq6r+DtM8qqFPsDnkNg5reWMGgmFfgxCRpSKyRUS2ichN0S5PT4nIShE5ICIbol2WSInIOBFZJSIbReQDEbmunz//ShH5j4j8WkRKgZtF5D4RuUtEnheRWuBkEZkqIqtFpMIt57KQbdwnIivceS3AdhH5cYfPSRORP4nIXhHZLSI/ExGPiMS7600PWXaEW+MZKSIZIvJ3ESkWkXJ3+NCPE+udn41HRN4Rkb/3x+f1FbcG+b6IrBeRQX2XTxFJF5HHRWSziGxyH7LWK4Z1QIiIB7gTOB04GrhIRI6Obql67D7chywNAX7gm6p6NLAA+EoUfi/HAjuAUcDP3WkXu8MpwH+BZ3EejTsSuBZ4UEQmh2zjAuCzgBeYDiwVkQUh8+/D2dcjgdnAqcDVqtoI/A3nyYoB5wP/p6oHcP5v7wUmAOOBeqC/mseuAzb102f1tZNVddYQuBbiN8CLqjoFKKAXfz/DOiCA+cA2Vd2hqk3AI8Cno1ymHlHVV4GyaJejN6jqXlVd5w5X4/zBj+2Dj3rK/aYeeF0TMm+Pqv5WVf2qWu9Oe1pV/+M++nYWkAzcoqpNqvoK8HfaH9SfVtWX3eVbcYJCAURkFHAG8HVVrXUP/L/GeYAWwEMhw+CE00MAqlqqqk+oap378/k58Ine+qF0xa2lnAlY5/cAISJpwEnAnwDcv8WK3tr+cO+DGAsUhowX4XxzNAOEiOThfLv+bx9s/uyD9EEUHmLaGKCww3PSP6Z9kBW6tdS1OLWEO1U1sB8TcAJjr7Q9OyEm5DNWAYkiciywHyeQngQQkUScMFkKZLjLp/RDP8ftwI04NajBToGXRESBP7rPtx+M8nGetnmviBTg/K1dp6q1vbHx4V6DMAOYiCQDT+B8y67q548Pd/536LQ9wDgRCf0fGk/756urqrao6iycZ6/PD+lXKAQagWxVTXdfqao6zV2xBXgMp0ZyEfB3t7YAzhMXJwPHqmoqzjdIgD57So+IfAo4oKpr++oz+tkJqnoMTvPyV0TkpEOtMEDFAscAd6nqbKAW6LW+1OEeELuBcSHjubT/BzdRIiJenHB4UFX/Fu3yhPFfoA64UUS8IrIIOAunmbITt9q/CrefSFX34vRf/EpEUkUkRkSOEJHQpqKHcPoxLnGHA1Jw+h0qRCQT+FEv7ldXFgLLRGQnzj6eIiJ/6YfP7ROqutt9P4BTM5sf3RL1WBFQFFIzfRwnMHrFcA+It4GjRCRfROJw2nyfiXKZhj1x2lz+BGxS1dv68KOe7XAdxJPdXdHtszoL5xtoCfB74HJV3RyyWKKIpAOISAKwBAidfzkQB2wEynH+uXNCPuO/ON8IxwAvhKx3O5Dgfu6bwIvdLXdPqep3VDVXVfNw/k9eUdVL+/pz+4KIJIlISmAY5+SAQXn2n6ruw2nKDJwcsRjn76lXDPsrqUXkDJx/OA+wUlV/fvA1BiYReRhYhHPb3/3Aj1T1T1EtVA+JyAnAv4H3cTp3Ab6rqs9Hr1SHT0RmAvfj/G3FAI+p6k+iW6rIubWlb6nqp6JclB4RkYm4/Tk4TTQPDdb/ewARmYVz4kAczpl3V6lqea9se7gHhDHGmPCGexOTMcaYLlhAGGOMCcsCwhhjTFhD5kK57OxszcvLi3YxjDFmUFm7dm1JV8+kHjIBkZeXx5o1g/qeW8YY0+9E5OOu5lkTkzHGmLAsIIwxZrBqboDda2Hna32y+SHTxGSMMUNaYw3s3wB73217FW+GVj/kFMAXXu31j7SAMMYMOM3NzRQVFdHQ0BDtokSHtkJLk/tqBn8TtDa7M/Ng9BEw9kKI9YInznltOvhjIHw+H7m5uXi93m4XwwLCGDPgFBUVkZKSQl5eHiG3Qx+aWpqhuR6a69xXvRMMAMRBTBJ4EyEuwXn3JkCMFw7j56KqlJaWUlRURH5+frfXs4Awxgw4DQ0NQy8cVN0wqAsJhPqQmgHgiXdCIDHbCQJvIngiP0yLCFlZWRQXFx/WehYQxpgBaVCHgyq0NLYFQZP7Hvo8p1gfxKe0BYHXBzF9d0juyc/TAsIYYyKhCv6G9rWC5jqnHwEAcQ7+CeltYRDrgxhPNEvdLXaaqzHGdFBRUcHvf//7zjO0FZrqoLYEKgqheAvsfc85m6jiY6gt5YwLr6aiyQNp4yB7MuTMhBFTIH08JI2AuKRBEQ5gNQhjjOkkEBBfvvqKtlpBUx34G/D7m4mNjQXxODWCpKy2zuNYH8+//H+9Xh6/3+98Zhfj3V3vcPVpQIjIUuA3OA9MuUdVb+kw/9fAye5oIjBSVdPdeS04D4wB2KWqy/qyrMaYgenHz37Axj29+0jyo8ek8qOzprVNaPW7TUNO89BNX/8y27dvY9bsY1hy0rGcuWQRP7j1LjIyMti8dQcfbtrA2Z+9kMLCQhoaGrjuuutYvnw50Hbbn5qaGk4//XROOOEEXn/9dcaOHcvTTz9NQkJCu7IUFxfzxS9+kV27dgFw++23s3DhQm6++Wa2b9/Ojh07GD9+PJMnT243/j//8z987nOfo6SkhBEjRnDvvfcyfvx4rrzySnw+H++88w4LFy7kttt6/lDGPgsIEfEAd+I8ZrEIeFtEnlHV4OPwVPUbIctfC8wO2US9+7B3Y4zpJeo0E/kboXpfW39B8LRSIMbLLT/6Nhs+3MH6NW+CN5HV//4P697byIYNG4Knia5cuZLMzEzq6+uZN28e5557LllZWe0+bevWrTz88MPcfffdnH/++TzxxBNcemn7J7Ved911fOMb3+CEE05g165dnHbaaWxyr2nYuHEjr732GgkJCdx8883txs866yyuuOIKrrjiClauXMnXvvY1nnrqKcA5Tfj111/H44msKasvaxDzgW2qugNARB4BPk3Xz0u9iP55+LoxZhBp902/K6pOLaClue3istYm8Lvjrc3ONEKeoFm917nAzJsAiSHNRB4vNOx0zihKSHeWFWH+/PntriG44447ePJJ58mlhYWFbN26tVNA5OfnM2vWLADmzJnDzp07OxX95ZdfZuPGtsNiVVUVNTU1ACxbtqxdjSN0/I033uBvf/sbAJdddhk33nhjcLnzzjsv4nCAvg2IsUBhyHgRcGy4BUVkApAPvBIy2SciawA/cIuqPhVmveXAcoDx48f3TqmNMQNPx4N/uHc6Pj5ZnIO9J87pGPbEtY0H3g/jtNKkpKTg8OrVq3n55Zd54403SExMZNGiRWGv+o6Pjw8Oezwe6uvrO+9aaytvvvkmPp/voJ8Zbrw7ZY3EQOmkvhB4XDX0JGEmqOpu9wHjr4jI+6q6PXQlVV0BrACYO3euPVzbmMGoqQ6qdkNlkfu+G1JOgNJtbQf/4CmjIWK8bTUAX1r4g38Pr6VISUmhurq6y/mVlZVkZGSQmJjI5s2befPNN3v0OQCnnnoqv/3tb7nhhhsAWL9+fbDWcTDHH388jzzyCJdddhkPPvggJ554Yo/L0JW+DIjdwLiQ8Vx3WjgXAl8JnaCqu933HSKyGqd/YnvnVY0xA5a/Car3OAf9QAiEBkFVEdSXd15v6RPQ2gKx8c7FZMGDfkgI9OGFdFlZWSxcuJDp06dz+umnc+aZZ7Yv3tKl/OEPf2Dq1KlMnjyZBQsW9Piz7rjjDr7yla8wc+ZM/H4/J510En/4wx8Oud5vf/tbrrrqKm699dZgJ3VvE9W++eItIrHAh8BinGB4G7hYVT/osNwU4EUgX93CiEgGUKeqjSKSDbwBfDq0g7ujuXPnqj0wyJh+1NridPR2/PZfVdQWCDUH6NT040uHtFxIHQtpY9333LZpqWPYtHUHU6dOjcZeDWmbNm3q9HMVkbWqOjfc8n1Wg1BVv4h8FfgHzmmuK1X1AxH5CbBGVZ9xF70QeETbJ9VU4I8i0opzMd8tBwsHY0wfam2Fki3OMweK3obyj52Df9We9reOAIhLbjvwj5rWIQhynfe43mkfN32vT/sgVPV54PkO037YYfzmMOu9Dszoy7IZY7rQ2uI8d2Dnf+Dj/8DHr0N9mTMvJQeyjoQJC9t/+w+EgC+9T5t+TP8aKJ3UxphoaWl2Hj6z8zUnDHa9CY2VzryMPJh8Bkw4HvIWQvoEC4BhxALCmOHG3+g8pvLj/zi1hMK3oLnWmZc9CaZ/xqkhTDjeqRWYYcsCwpihrqkOit5yagc7/+P0I7Q0OvNGTYfZl7QFQvLI6JbVDCgWEMYMNQ1VUPjfthrCnnXOhWYS4zy7eP41TiCMXwCJmdEurRnALCCMGezqymDXG24N4TXY955zYVmMF8YeA8dfCxNOgHHzwZca7dIOChUVFTz00EN8+ctf7tH6t99+O8uXLycxMbGXS9a/LCCMGWxqDrSdXbTzP3DAvbQo1ge58+CkG5waQu48iBvcB6hoCd7uO4KAuPTSS3scED29vXdLS0uv3IMpwALCmIGucrcbCG6TUelWZ7o3CcYfC9PPcWoIY49xrjweal64Cfa9f+jlDsfoGXD6LV3Ovummm9i+fTuzZs1iyZIl3Hrrrdx666089thjNDY2cs455/DjH/+Y2tpazj//fIqKimhpaeEHP/gB+/fvZ8+ePZx88slkZ2ezatWqdtteu3Yt119/PTU1NWRnZ3PfffeRk5PDokWLmDVrFq+99hoXXXQRzz77bLvxWbNm8a1vfQu/38+8efO46667iI+PJy8vjwsuuIB//vOf3HjjjVx44YW99mOygDBmIFGF8p0hNYTXnCeVAcSnOf0Gx1zmBELOTOeWE6bX3XLLLWzYsIH169cD8NJLL7F161beeustVJVly5bx6quvUlxczJgxY3juuecA5x5NaWlp3HbbbaxatYrs7Ox2221ububaa6/l6aefZsSIETz66KN873vfY+XKlQA0NTURuCPEs88+GxxvaGjgqKOO4l//+heTJk3i8ssv56677uLrX/864NwaZN26db3+c7CAMCaaVKFkK3z8WluTUfUeZ15ilnNm0YIvOe+jpg+aR1X2qoN80+8vL730Ei+99BKzZzuPrKmpqWHr1q2ceOKJfPOb3+Tb3/42n/rUpw55w7wtW7awYcMGlixZAjhNQjk5OcH5F1xwQbvlA+NbtmwhPz+fSZMmAXDFFVdw5513BgOi43q9xQLCmP7U2goHNjphEAiF2mJnXvIop+8gb6Hznj0ZYuyx8QOBqvKd73yHL3zhC53mrVu3jueff57vf//7LF68mB/+8IdhttC2nWnTpvHGG2+EnR/t23t3ZAFhTF9qqnMCYdebbc1GDRXOvLTxcMTitkDInGhXKQ8QHW/3fdppp/GDH/yASy65hOTkZHbv3o3X68Xv95OZmcmll15Keno699xzT7v1OzYxTZ48meLiYt544w2OO+44mpub+fDDD5k27eAPRZo8eTI7d+5k27ZtHHnkkfz5z3/mE5/4RO/veAcWEMb0lpoDzimm+95ve5Vua3uWQeYRMPUsyDvBaTJKt4dcDVQdb/d96623smnTJo477jgAkpOT+ctf/sK2bdu44YYbiImJwev1ctdddwGwfPlyli5dypgxY9p1UsfFxfH444/zta99jcrKSvx+P1//+tcPGRA+n497772X8847L9hJ/cUvfrHvfgCuPrvdd3+z232bftPaAmU7OodBzf62ZdLGO2fKBF5j50BqTtfbNO2Euy21idyAud23MUNCUy3s3+iEwf4NThDs/8B50D04F6ONnAJHfrItDEZNg4SM6JbbmF5gAWFMQPV+tzbwXvsmosADb3xpMHomzLmyLQyyJ0NsXDRLbUyfsYAww09rC5Ru79xEVHugbZn0CU4AzDgPRk93htPGWSdyP1JVxH7evaYn3QkWEEOFqnNPnvoy5xm+vnTw+qJdquhrrHHOItr3HuwLaSLy1zvzY7wwciocdWqHJqL0qBZ7uPP5fJSWlpKVlWUh0QtUldLSUny+wzsmWEAMBi1+qNkHVXudRz1Wu+9Ve53HPlbvcYYDt3AOiE1wDnS+9PbvCRmdp/nc6YHhwdZsoup0EndqItpOWxNRuhMAcz8X0kQ0afDt6zCQm5tLUVERxcXF0S7KkOHz+cjNzT2sdQ4ZECISAyxwHwNqeltTnXvA3xNysO/wqj3QdqpkgCfeOSsmdSyMnQtTx0DqGEjIhKZqqK9wzrcPfa8scjpa6yucZQ7Gm9h1eBwqbPr69g+tLU7fQMcwqA05mGTkOQEw84K2MEgda01Eg4TX6yU/Pz/axRj2DhkQqtoqIncCsw934yKyFPgN4AHuUdVbOsy/ErgV2O1O+p2q3uPOuwL4vjv9Z6p6/+F+flSpQn15+4N/uwBwawGBi6ZCxac5B/vUHBh1NKS4B//AK2WMcx//SA52LX5oqGwLj/pyd7i8c7DUVzj3A9r7rjOtqebg2/YmHSRQ0tsCp2MA+dLA0+FPsrHGaRIKBMH+Dc5ZRYEmIk+c00Q06TSnAznQRORL6/nPxhgDdL+J6V8ici7wN+1mT4eIeIA7gSVAEfC2iDyjqhs7LPqoqn61w7qZwI+AuTjtA2vddcu7Wd6+1driXBTV8WDfMQwCB7EgcZ7YlZLjfMOdcFzbAT948M+B+OS+3wdPLCRlOa/D1dLshEt9efiaSseQKfuobTzwaMuuxKW0hUZznXO9QaCJKCHDCYF5n2/fRGQ3rDOmT3Q3IL4AXA+0iEg9IICq6sGePjIf2KaqOwBE5BHg00DHgAjnNOCfqlrmrvtPYCnwcDfL23PNDc6BPniw3905AKr3gba0Xy/G29bkk1MAk09v/40/dQykjB4aBzOPF5Kyndfh8jd1DpSugiUmFgouCmkiGmNNRMb0o24FhKqm9GDbY4HCkPEi4Ngwy50rIicBHwLfUNXCLtbt9PR0EVkOLAcYP76Hty2oLYGnvtxWG6gr7bxMXEpbk8/ERc63/E5NPll2Y7XuiI1zalH27GNjBrxun8UkIsuAk9zR1ar69174/GeBh1W1UUS+ANwPnNLdlVV1BbACnFtt9KgE3kQnHFLHOk/gCjb3uLWBlBx7TKMxZljqVkCIyC3APOBBd9J1IrJQVb9zkNV2A+NCxnNp64wGQFVDv67fA/wiZN1FHdZd3Z2yHra4RPjia32yaWOMGcy62yZyBrBEVVeq6kqc/oAzD7HO28BRIpIvInHAhcAzoQuISOjdy5YBm9zhfwCnikiGiGQAp7rTjDHG9JPDuVAuHShzhw95DqGq+kXkqzgHdg+wUlU/EJGfAGtU9Rnga27Tld/d9pXuumUi8lOckAH4SaDD2hhjTP/o1u2+ReRC4H+BVThnMJ0E3KSqj/Zt8brPbvdtjDGHL6LbfbtXUrcCC3D6IQC+rar7eq+IxhhjBpruXkl9o6o+Roc+BGOMMUNXdzupXxaRb4nIOBHJDLz6tGTGGGOiqrud1Be4718JmabAxN4tjjHGmIGiu30QA6pD2hhjTN87ZBOTqrYCN/RDWYwxxgwg1gdhjDEmLOuDMMYYE1Z37+Zqj3Yyxphh5qBNTCJyY8jweR3m/b++KpQxxpjoO1QfxIUhwx3v3Lq0l8tijDFmADlUQEgXw+HGjTHGDCGHCgjtYjjcuDHGmCHkUJ3UBSJShVNbSHCHccd9fVoyY4wxUXXQgFBVT38VxBhjzMDS3QvljDHGDDMWEMYYY8KygDDGGBOWBYQxxpiw+jQgRGSpiGwRkW0iclOY+deLyEYReU9E/iUiE0LmtYjIevdlT7Izxph+1t2b9R02EfEAdwJLgCLgbRF5RlU3hiz2DjBXVetE5EvAL2i7MWC9qs7qq/IFNPpbuP6xd8lJ9TE6zXnlpPkYleq8vB6rZBljhqc+CwhgPrBNVXcAiMgjwKeBYECo6qqQ5d8ELu3D8oRVWd/Mpj1VvLLpAPXNLe3miUB2cjyjU9sHR44bJKNTfeSkJZAQZ2cDG2OGnr4MiLFAYch4EXDsQZb/PPBCyLhPRNYAfuAWVX2q4woishxYDjB+/PgeFXJkio9XvrUIVaWqwc++ygb2Vtazv6qBvZUN7KtsYF9VA4Vldbz1URmV9c2dtpGW4O06RNJ85KQmkJoQi4jdncQYM3j0ZUB0m4hcCswFPhEyeYKq7haRicArIvK+qm4PXU9VVwArAObOnRvRrT9EhLQEL2kJXiaPTulyufqmFvZVdQ6RvZUN7K9qYOPeKkpqGtEOpUnwekJqHT5GuWEyOqRpKzspnpgYCxFjzMDQlwGxGxgXMp7rTmtHRD4JfA/4hKo2Bqar6m73fYeIrAZmA9s7rt/fEuI85GcnkZ+d1OUyzS2tHKhuZF9lfVstpLKBvVUN7K9s4L8flbG/qgF/a/sUiY0RRoUExuiQmkigZjIyxUdcrPWLGGP6Xl8GxNvAUSKSjxMMFwIXhy4gIrOBPwJLVfVAyPQMoE5VG0UkG1iI04E9KHg9MYxNT2BsekKXy7S2KqW1TV02aR2sXyQrKb5dP8jokNrIqDQfI1PiSY63Ji1jTGT6LCBU1S8iXwX+AXiAlar6gYj8BFijqs8AtwLJwF/dg9kuVV0GTAX+KCKtOKfi3tLh7KdBLyZGGJESz4iUeGbkpoVdRlWpqvcHm7QC4RFo0jpYv0iC1xPc/kj3fUSyO54az4hkHyNS4slKjrMztYwxYYl2bCwfpObOnatr1qyJdjGiomO/SHF1IweqGimuaaS42nkdqG4MGyQikJkYFwyTESFhMjLV1y5UUqxWYsyQIyJrVXVuuHkDopPaRKY7/SLgXPNRUtPEATdEAgFyoLotSHYU11Jc3UhTS2un9eNjY8LUSnxujaQtXLKT462fxJghwAJiGImP9RyybwTamraKaxra1URCg2RnidO8VV7XuVYCkJHodYPE16FWEtLUleKz03+NGcAsIEwnIkJaope0RC9Hjuz6lF+AJn8rpbWNnZq1DlQ3BMNkzce1HKhqpNHfuVYS54nponkrnqykeDISvWQkxZGRGEd6otf6S4zpRxYQJiJxsTHkpCWQk3boWkl1o7+L/hEnTArL6lj3cTlldU2driMJSImPJT3J6wZGnBMgiU6AZCR5201LT/SSmRRHgtdjtRRjesACwvQLESHV5yXV5+WIEckHXba5pZWy2iaKqxupqGumvK6JiromyoPDznt5bRM7S2opr2uiusHf5fbiYmPahUYgXDI7BE1ouKQleO2iRTPsWUCYAcfriQneLLG7mltaqaxvpqKuibLaMKFS20yZO23rgZrgvJbW8FWVGHFuodIxVEKbvIKhktS2XHys3ZfLDB0WEGZI8HpiyE52zqDqrkCzVyA8gqFS2xwMkECo7K1sYNPeKsrrmjtdvBgqKc7TITTagiQ9wes0gyXEkZbodcYT40hN8OKx2ooZgCwgzLAV2uw1Piux2+s1NLe4TVzNHZq+3OFaJ2zK65opLKujvK6ZqobmLvtVAFJ9sU6IhIRJuhsibdO9pCW01WhSfbHEWqe96UMWEMYcJp/X062O+VAtrUpVfTMVbjNY8L2umYq65mDzWHmds8yu0loq6p3pBwuWFF+sGyRhwqXdsBMuGYnODSktWEx3WEAY0w88MeL0XSTFAQe/oDFUa6tS1eCESCBUKuudWooz3j5cisrrg8t00b0COGeDpYV03KcFayxdBI27jJ1mPLxYQBgzgMXEiHuAjjus9VpbleoGPxX1Te3CJVBjqahvotKdXl7XxO7y+uAyBwuW5PhYkuI9JMXFkhDnvCe2G/eQGB/rvMfFkthhPCne40yLiw2ua6EzcFlAGDMExcS0Xew4Iav767W2Oh33lW6IhAuXuiY/tU0t1DX6qWtqoby2iaLyeuoa3elNfppbun+PtzhPTKdwaQufjmHjTo/zkBQfElLuuBM+zvLW8R85CwhjTFBMTNuDs8bT/Y77jpr8rdQ3tVDb5HcCpbGFOjc8AuFS29RCfYfxuiYndOoanRtQ1jW1UNvoD27rYLWbjnzemLA1nUCIJPtiyUyMIzMpjszkeLKSnOEstynQajYWEMaYPhAXG0NcbAxpid5e26aq0uhvpdatudSGhEloENWHzAssWxcyXlLTSG2Tn+oG/0FPAkj1xZKVHO8EiBscweHkODKT4ttN83mH3jUwFhDGmEFBRPB5Pfi8Hg6j1eyg/C2tVNQ3U1bbRGlNE2W1TZTVNlJa6wyX1jZRVtPErtI63tlVQXldU5cXVybFecjsEByB94yQ4aykeDKT40iKG/i3gLGAMMYMW7GhF1iOOvTygbPKggHihkp5XWDYCZf9Vc6FlaW1TTSFuUklOLWsdrWSJDdckjtOc0IlGnc+toAwxphuCj2r7IgRh15eValtaqGsponS2sa2Wkm7gHGmf1RSS1ltE3VN4a/Uj3VPlQ4XKvkjklhWMKaX99YCwhhj+oyIkBwfS3J8bLev1m9obgk2bQVCJbS5q9RtBtuwu5LSWudGlXMmZFhAGGPMUOfzdu/BXgFN/taD3h8sEhYQxhgziAXOGOsLdqKvMcaYsCwgjDHGhCV6sFtFDiIiUgx8HMEmsoGSXipONA2V/QDbl4FqqOzLUNkPiGxfJqhq2HOyhkxAREpE1qjq3GiXI1JDZT/A9mWgGir7MlT2A/puX6yJyRhjTFgWEMYYY8KygGizItoF6CVDZT/A9mWgGir7MlT2A/poX6wPwhhjTFhWgzDGGBOWBYQxxpiwhn1AiMhSEdkiIttE5KZol6enRGSliBwQkQ3RLkukRGSciKwSkY0i8oGIXBftMvWEiPhE5C0Redfdjx9Hu0yREhGPiLwjIn+PdlkiISI7ReR9EVkvImuiXZ5IiEi6iDwuIptFZJOIHNdr2x7OfRAi4gE+BJYARcDbwEWqujGqBesBETkJqAEeUNXp0S5PJEQkB8hR1XUikgKsBc4ebL8XcW7en6SqNSLiBV4DrlPVN6NctB4TkeuBuUCqqn4q2uXpKRHZCcxV1UF/oZyI3A/8W1XvEZE4IFFVK3pj28O9BjEf2KaqO1S1CXgE+HSUy9QjqvoqUBbtcvQGVd2rquvc4WpgEzA2uqU6fOqocUe97mvQfiMTkVzgTOCeaJfFOEQkDTgJ+BOAqjb1VjiABcRYoDBkvIhBeCAaykQkD5gN/DfKRekRt0lmPXAA+KeqDsr9cN0O3AiEf0Ta4KLASyKyVkSWR7swEcgHioF73aa/e0Qkqbc2PtwDwgxgIpIMPAF8XVWrol2enlDVFlWdBeQC80VkUDb/icingAOqujbaZeklJ6jqMcDpwFfcJtrBKBY4BrhLVWcDtUCv9aUO94DYDYwLGc91p5koc9vsnwAeVNW/Rbs8kXKr/auApVEuSk8tBJa5bfePAKeIyF+iW6SeU9Xd7vsB4Emc5ubBqAgoCqmZPo4TGL1iuAfE28BRIpLvdu5cCDwT5TINe27n7p+ATap6W7TL01MiMkJE0t3hBJyTITZHtVA9pKrfUdVcVc3D+T95RVUvjXKxekREktyTH3CbY04FBuXZf6q6DygUkcnupMVAr53MMayfKKeqfhH5KvAPwAOsVNUPolysHhGRh4FFQLaIFAE/UtU/RbdUPbYQuAx4322/B/iuqj4fvSL1SA5wv3u2XAzwmKoO6tNDh4hRwJPO9xBigYdU9cXoFiki1wIPul9ydwBX9daGh/VprsYYY7o23JuYjDHGdMECwhhjTFgWEMYYY8KygDDGGBOWBYQxxpiwLCCMOQwi0uLeATTw6rWrVkUkbyjcjdcMHcP6OghjeqDevXWGMUOe1SCM6QXu8wV+4T5j4C0ROdKdnicir4jIeyLyLxEZ704fJSJPus+KeFdEjnc35RGRu93nR7zkXoFtTFRYQBhzeBI6NDFdEDKvUlVnAL/DufMpwG+B+1V1JvAgcIc7/Q7g/1S1AOfeOYEr+I8C7lTVaUAFcG6f7o0xB2FXUhtzGESkRlWTw0zfCZyiqjvcGw3uU9UsESnBefhRszt9r6pmi0gxkKuqjSHbyMO5JfhR7vi3Aa+q/qwfds2YTqwGYUzv0S6GD0djyHAL1k9oosgCwpjec0HI+xvu8Os4dz8FuAT4tzv8L+BLEHyoUFp/FdKY7rJvJ8YcnoSQO8wCvKiqgVNdM0TkPZxawEXutGtxnvZ1A86TvwJ32rwOWCEin8epKXwJ2NvXhTfmcFgfhDG9wO2DmKuqJdEuizG9xZqYjDHGhGU1CGOMMWFZDcIYY0xYFhDGGGPCsoAwxhgTlgWEMcaYsCwgjDHGhPX/AW4E+SL9L183AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Check Fitting\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
    "        :param history: Training history of model\n",
    "        :return:\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    # create accuracy sublpot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "\n",
    "    # create error sublpot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
